{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/htkumar/llms/mistral-finetune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3e6c0ae7c2484da4e4dd3198264cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model.args import ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xformers.ops.fmha import memory_efficient_attention\n",
    "from xformers.ops.fmha.attn_bias import AttentionBias, BlockDiagonalCausalMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int,\n",
    "        scaling: float,\n",
    "        dropout: float,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.scaling = scaling\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        assert not bias\n",
    "        self.bias = bias\n",
    "\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=self.bias)\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=self.bias)\n",
    "        self.frozen_w = nn.Linear(in_features, out_features, bias=self.bias)\n",
    "\n",
    "        def ignore_missing_keys(m: nn.Module, incompatible_keys: NamedTuple):\n",
    "            incompatible_keys.missing_keys[:] = []\n",
    "\n",
    "        self.register_load_state_dict_post_hook(ignore_missing_keys)\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        lora_res = self.lora_B(self.lora_A(self.dropout(x)))\n",
    "        return self.frozen_w(x) + lora_res * self.scaling\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"LorA Linear(in_features: {self.in_features}, out_features: {self.out_features}, rank: {self.rank}, scaling: {self.scaling}, dropout: {self.dropout})\"\n",
    "\n",
    "    def merge_weight(self):\n",
    "        with torch.no_grad():\n",
    "            down_weight = self.lora_A.weight\n",
    "            up_weight = self.lora_B.weight\n",
    "            lora_weight = up_weight.mm(down_weight) * self.scaling\n",
    "\n",
    "            weight += self.frozen_w.weight\n",
    "            return weight\n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        prefix,\n",
    "        local_metadata,\n",
    "        strict,\n",
    "        missing_keys,\n",
    "        unexpected_keys,\n",
    "        error_msgs,\n",
    "    ):\n",
    "        key_name = prefix + \"weight\"\n",
    "        if key_name in state_dict:\n",
    "            w_ref = state_dict[key_name]\n",
    "\n",
    "            self.frozen_w.load_state_dict({\"weight\": w_ref}, assign=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features, out_features, rank = 128, 256, 16\n",
    "frozen_w = nn.Linear(in_features, out_features, bias=False)\n",
    "lora_A = nn.Linear(in_features, 16, bias=False)\n",
    "lora_B = nn.Linear(16, out_features, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 128]), torch.Size([256, 16]), torch.Size([256, 128]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_A.weight.shape, lora_B.weight.shape, frozen_w.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_linear = LoRALinear(\n",
    "    in_features=128, bout_features=256, rank=16, scaling=1.0, dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_linear(torch.randn(16, 128)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "mistral_model_path = Path.home().joinpath(\"mistral_models\", \"7B-v0.3\")\n",
    "mistral_model_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 792151339759898,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "mistral_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(128, 128)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.w1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experts = [nn.Linear(128, 128) for _ in range(8)]\n",
    "gate = nn.Linear(128, 8, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(4, 12, 128)\n",
    "input = input.view(-1, input.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_logits = gate(input)\n",
    "gate_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, selected_experts = torch.topk(gate_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape, selected_experts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0, 0, :], selected_experts[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_s = F.softmax(weights, dim=1)\n",
    "weights_s[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = torch.zeros_like(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx, nth_expert = torch.where(selected_experts == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx, nth_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_experts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape, input[batch_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[batch_idx, nth_expert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[batch_idx, nth_expert, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experts[0](input[batch_idx]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(\n",
    "    (weights[2, 0, None] * experts[0](input[2])), (weights[2, 0] * experts[0](input[2]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 438757202459634,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_json(\n",
    "    \"/home/htkumar/llms/mistral-finetune/ultrachat/train.jsonl\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = train_data.iloc[3674]\n",
    "print(first_sample[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_sample[\"prompt_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from simple_parsing.helpers import Serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MoeArgs(Serializable):\n",
    "    num_experts: int = 8\n",
    "    num_experts_per_tok: int = 2\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.num_experts > 10:\n",
    "            raise ValueError(\"num_experts must be <= 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = MoeArgs(num_experts=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/htkumar/llms/mistral-finetune/moe_args.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(a.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/htkumar/llms/mistral-finetune/moe_args.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        b = MoeArgs.from_dict(json.loads(line))\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "016c5a11-f387-442a-b9a5-bfad5df34987",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "mistral_ft (local)",
   "language": "python",
   "name": "mistral_ft_local"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
