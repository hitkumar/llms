{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "# del variables\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from modeling_gemma import PaliGemmaForConditionalGeneration, PaliGemmaConfig\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "import glob\n",
        "from safetensors import safe_open\n",
        "from typing import Tuple\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from processing_paligemma import PaliGemmaProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.tensor([12]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_tokens = [torch.tensor([12]), torch.tensor([13]), torch.tensor([14]), torch.tensor([15])]\n",
        "generated_tokens = torch.cat(generated_tokens, dim=-1)\n",
        "generated_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from paligemma_utils import load_hf_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = \"/home/htkumar/paligemma-3b-pt-224\"\n",
        "device = 'cuda'\n",
        "model, tokenizer = load_hf_model(model_path, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.config.vision_config.num_image_tokens, model.config.vision_config.image_size)\n",
        "224 / 14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.config.text_config.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_image_tokens = model.config.vision_config.num_image_tokens\n",
        "image_size = model.config.vision_config.image_size\n",
        "processor = PaliGemmaProcessor(tokenizer, num_image_tokens, image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from inference import test_inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# image_file_path='/home/htkumar/llms/paligemma-pytorch/beans_1000_leaf.jpeg'\n",
        "# prompt='This leaf is '\n",
        "# image = Image.open(image_file_path)\n",
        "# model_inputs = processor([prompt], [image])\n",
        "# model_inputs['input_ids'].dtype, model_inputs['pixel_values'].dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_inference(\n",
        "    model=model,\n",
        "    processor=processor,\n",
        "    prompt='the number of leaves is ',\n",
        "    image_file_path='/home/htkumar/llms/paligemma-pytorch/beans_1000_leaf.jpeg',\n",
        "    max_tokens_to_generate=10,\n",
        "    device=device,\n",
        "    temperature=0.0,\n",
        "    top_p=0.95,\n",
        "    do_sample=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Paligemma2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pl2_model_path = \"/home/htkumar/models--google--paligemma2-3b-pt-224/snapshots/96eeb174da13ca1a2b247e4d0867436296c36420\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(pl2_model_path, padding_side=\"right\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "model, tokenizer = load_hf_model(pl2_model_path, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.config.vision_config.num_image_tokens, model.config.vision_config.image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_image_tokens = model.config.vision_config.num_image_tokens\n",
        "image_size = model.config.vision_config.image_size\n",
        "processor = PaliGemmaProcessor(tokenizer, num_image_tokens, image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from inference import test_inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_file_path='/home/htkumar/llms/paligemma-pytorch/beans_1000_leaf.jpeg'\n",
        "prompt='This leaf is '\n",
        "image = Image.open(image_file_path)\n",
        "model_inputs = processor([prompt], [image])\n",
        "model_inputs['input_ids'].dtype, model_inputs['pixel_values'].dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_inference(\n",
        "    model=model,\n",
        "    processor=processor,\n",
        "    prompt='the color of the leaf is ',\n",
        "    image_file_path='/home/htkumar/llms/paligemma-pytorch/beans_1000_leaf.jpeg',\n",
        "    max_tokens_to_generate=10,\n",
        "    device=device,\n",
        "    temperature=0.0,\n",
        "    top_p=0.95,\n",
        "    do_sample=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scratch pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = tokenizer([\"How are you doing today?\", \"Hey there\"], return_tensors='pt', padding='longest', truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{'b': 2, **a}['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.all_special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len([v for v in tokenizer.get_vocab() if \"<loc\" in v])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"beans\", split=\"train\")\n",
        "print(len(ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds[1000]['image'].save('/home/htkumar/llms/paligemma-pytorch/beans_1000_leaf.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = Image.open('/home/htkumar/llms/paligemma-pytorch/beans_1000_leaf.jpeg')\n",
        "np.array(image).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from processing_paligemma import process_images, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.array(image).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "process_images([image], size=(244, 244), rescale_factor=1 / 255.0, image_mean=IMAGENET_STANDARD_MEAN, image_std=IMAGENET_STANDARD_STD)[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Image\n",
        "dataset = load_dataset(\"ILSVRC/imagenet-1k\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = \"/home/htkumar/paligemma-3b-pt-224\"\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
        "assert tokenizer.padding_side == \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.encode(\"How are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.max_len_single_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "safetensor_files = glob.glob(os.path.join(model_path, \"*.safetensors\"))\n",
        "safetensor_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tensors = {}\n",
        "for safetensors_file in safetensor_files:\n",
        "    with safe_open(safetensors_file, framework=\"pt\", device='cpu') as f:\n",
        "        for key in f.keys():\n",
        "            tensors[key] = f.get_tensor(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model config\n",
        "with open(os.path.join(model_path, \"config.json\"), \"r\") as f:\n",
        "    model_config_file = json.load(f)\n",
        "    config = PaliGemmaConfig(**model_config_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = PaliGemmaForConditionalGeneration(config).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(tensors, strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.load_state_dict??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.tie_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.language_model.model.embed_tokens.weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target = torch.tensor([1, 2, 3, 4, 5])\n",
        "mask = torch.tensor([False, True, False, True, False])\n",
        "source = torch.tensor([10, 20])\n",
        "\n",
        "result = target.masked_scatter(mask, source)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attention_mask = torch.full(\n",
        "    (4, 12), fill_value=1\n",
        ")\n",
        "attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attention_mask.cumsum(-1)[:, -1].dim()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(attention_mask.cumsum(-1)).masked_fill_((attention_mask == 0), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size, hidden_state = 10000, 256\n",
        "lm_head = nn.Linear(hidden_state, vocab_size)\n",
        "embed_tokens = nn.Embedding(vocab_size, hidden_state)\n",
        "lm_head.weight.shape, embed_tokens.weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.equal(lm_head.weight, embed_tokens.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lm_head.weight = embed_tokens.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.equal(lm_head.weight, embed_tokens.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base, dim = 1000, 128\n",
        "inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "inv_freq.shape, inv_freq.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inv_freq_expanded = inv_freq[None, :, None]\n",
        "inv_freq_expanded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bs, seq_len, n_heads = 2, 10, 4\n",
        "position_ids = torch.cat([torch.arange(seq_len)[None, :] for _ in range(bs)], dim=0)\n",
        "position_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "position_ids_expanded = position_ids[:, None, :].float()\n",
        "position_ids_expanded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n",
        "freqs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = torch.ones(bs, n_heads, seq_len, dim)\n",
        "q.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1 = q[..., :q.shape[-1] // 2]\n",
        "x2 = q[..., q.shape[-1] // 2:]\n",
        "x1.shape, x2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.cat((-x2, x1), dim=-1)\n",
        "a.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freqs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emb = torch.cat((freqs, freqs), dim=-1)\n",
        "emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cos = emb.cos()\n",
        "cos.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cos = cos.unsqueeze(1)\n",
        "cos.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rotate_half(x):\n",
        "    x1 = x[..., :x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2:]\n",
        "    return torch.cat((-x2, x1), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rotate_half(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.arange(10).expand(1, -1)\n",
        "a.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_positions = 100\n",
        "emb_dim = 128\n",
        "B = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emb_layer = nn.Embedding(num_positions, emb_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv_output = torch.randn(B, num_positions, emb_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "position_ids = torch.arange(num_positions)\n",
        "position_emb = emb_layer(position_ids)\n",
        "position_emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_1 = conv_output + position_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_2 = conv_output + emb_layer(position_ids.expand(1, -1))\n",
        "out_2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.equal(out_1, out_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "B, num_heads, seq_len, emb_dim = 4, 2, 10, 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attn_scores = torch.randn(B, num_heads, seq_len, seq_len)\n",
        "values = torch.randn(B, num_heads, seq_len, emb_dim // num_heads)\n",
        "attn_scores.shape, values.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out = attn_scores @ values\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "act = torch.ones(B, seq_len, emb_dim)\n",
        "act.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "act.mean(-1, keepdim=True).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "6722272a-6af9-4944-b1b3-6cfb3ad11568",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "gpt-dev (local)",
      "language": "python",
      "name": "gpt-dev_local"
    },
    "language_info": {
      "name": "python"
    }
  }
}
