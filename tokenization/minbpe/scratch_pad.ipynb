{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    \n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from unicodedata import category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = '\\n'\n",
    "f\"\\\\u{ord(ch):04x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_control_characters(s: str) -> str:\n",
    "    chars = []\n",
    "    for ch in s:\n",
    "        if unicodedata.category(ch)[0] != 'C':\n",
    "            chars.append(ch)\n",
    "        else:\n",
    "            chars.append(f\"\\\\u{ord(ch):04x}\") # escape\n",
    "    \n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_control_characters(\"abcd\\ne\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_token(t: bytes) -> str:\n",
    "    s = t.decode('utf-8', errors='replace')\n",
    "    return replace_control_characters(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding individual characters\n",
    "# Encoding the character 'A' (U+0041)\n",
    "utf8_encoded_A = b'\\x41'\n",
    "print(utf8_encoded_A)  # Output: b'A'\n",
    "\n",
    "# Encoding the Euro sign 'â‚¬' (U+20AC)\n",
    "utf8_encoded_euro = b'\\xe2\\x82\\xac'\n",
    "print(utf8_encoded_euro)  # Output: b'\\xe2\\x82\\xac'\n",
    "\n",
    "# Encoding the emoji 'ðŸ˜Š' (U+1F60A)\n",
    "utf8_encoded_emoji = b'\\xf0\\x9f\\x98\\x8a'\n",
    "print(utf8_encoded_emoji)  # Output: b'\\xf0\\x9f\\x98\\x8a'\n",
    "\n",
    "\n",
    "# Encoding a string\n",
    "# Encoding the string 'Hello, world!' in UTF-8\n",
    "utf8_encoded_string = 'Hello, world!'.encode('utf-8')\n",
    "print(utf8_encoded_string)  # Output: b'Hello, world!'\n",
    "\n",
    "# Encoding a string with characters from multiple scripts\n",
    "# Encoding the string 'ä½ å¥½, world!' containing Chinese characters (U+4F60 U+597D)\n",
    "utf8_encoded_multilingual_string = 'ä½ å¥½, world!'.encode('utf-8')\n",
    "print(utf8_encoded_multilingual_string)  # Output: b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd, world!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(utf8_encoded_multilingual_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utf8_encoded_multilingual_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.pow(2, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.log(65535)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding utf-8 encoding (smiley is reprsented using 4 bytes, registered using 2 and euro using 3)\n",
    "# each number in the output list is [0, 255]\n",
    "a = '\\n\\r\\rðŸ˜ŠÂ®â‚¬'\n",
    "#  list(b\"\".join([a.encode('utf-8')]))\n",
    "list(a.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_control_characters('\\n\\rabcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_token(a.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "a[(1, 1)] = 2\n",
    "a[(2, 2)] = 3\n",
    "a[(3, 3)] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx1, idx2 in a:\n",
    "    print(idx1, idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in a.items():\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "p = Path('abcd')\n",
    "model_file = p.with_suffix('.model')\n",
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_file, 'w') as f:\n",
    "    f.write('abcdefg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"Base class for tokenizer\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # default vocab size is 256 (same as ascii chars), no merges, no patterns\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.pattern = \"\" # str\n",
    "        self.special_tokens = {} # str -> int eg. {'<|endoftext|>': 1}\n",
    "        self.vocab = self._build_vocab() # int -> bytes\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        vocab = {idx: bytes(idx) for idx in range(256)}\n",
    "        # the fact that iteration order is same as order in which items are inserted is key here, otherwise we don't have vocab entries for previous merges\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode('utf-8')\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def encode(self, text):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def save(self, file_prefix):\n",
    "        \"\"\"\n",
    "        Saves two files: file_prefix.vocab and file_prefix.model\n",
    "        Similar to sentencepiece\n",
    "        - model file is used for model loading, vocab is just for human viz.\n",
    "        \"\"\"\n",
    "        file = Path(file_prefix)\n",
    "        model_file = file.with_suffix('.model')\n",
    "        with open(model_file, 'w') as f:\n",
    "            # write version, pattern and merges\n",
    "            f.write('minbpe v1\\n')\n",
    "            f.write(f\"{self.pattern}\\n\")\n",
    "            # special tokens\n",
    "            f.write(f\"{len(self.special_tokens)}\\n\")\n",
    "            for special, idx in self.special_tokens.items():\n",
    "                f.write(f\"{special} {idx}\\n\")\n",
    "            \n",
    "            # merges dict\n",
    "            for idx1, idx2 in self.merges: # write only the ids of the merge\n",
    "                f.write(f\"{idx1} {idx2}\\n\")\n",
    "        \n",
    "        # write the vocab, for human viz\n",
    "        # vocab file is different than actual vocab, file is lossy but self.vocab is good.\n",
    "        vocab_file = file.with_suffix('.vocab')\n",
    "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
    "        with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "            for idx, token in self.vocab.items():\n",
    "                # replaces some partial utf-8 seq into ? token, so this can't be decoded due to error = 'replace'\n",
    "                s = render_token(token)\n",
    "                if idx in inverted_merges:\n",
    "                    idx0, idx1 = inverted_merges[idx]\n",
    "                    s0 = render_token(self.vocab[idx0])\n",
    "                    s1 = render_token(self.vocab[idx1])\n",
    "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\") # we should be able to change this\n",
    "                else:\n",
    "                    # print the bytes and special characters, double check the special characters part.\n",
    "                    f.write(f\"[{s}] {idx}\")\n",
    "    \n",
    "    def load(self, model_file):\n",
    "        \"\"\"Invert the functionality in save, but only for model file\"\"\"\n",
    "        assert model_file.endswith(\".model\")\n",
    "        # read the model file\n",
    "        merges = {}\n",
    "        special_tokens = {}\n",
    "        idx = 256\n",
    "\n",
    "        with open(model_file, 'r', encoding='utf-8') as f: # this is decoding, but understand this part more.\n",
    "            version = f.readline().strip()\n",
    "            assert version == \"minbpe v1\"\n",
    "            self.pattern = f.readline().strip()\n",
    "            num_special = int(f.readline().strip())\n",
    "            for _ in range(num_special):\n",
    "                special, special_idx = f.readline().strip().split()\n",
    "                special_tokens[special] = int(special_idx)\n",
    "\n",
    "            # read the merges\n",
    "            for line in f:\n",
    "                idx1, idx2 = map(int, line.split())\n",
    "                merges[(idx1, idx2)] = idx\n",
    "                idx += 1\n",
    "        \n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab = self._build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "        ids = list(text.encode('utf-8'))\n",
    "\n",
    "        merges = {}\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        idx = 256\n",
    "        for i in range(num_merges):\n",
    "            stats = get_stats(ids)\n",
    "            top_pair = max(stats, key=stats.get)\n",
    "            ids = merge(ids, top_pair, idx)\n",
    "            merges[top_pair] = idx\n",
    "            vocab[idx] = vocab[top_pair[0]] + vocab[top_pair[1]]\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{merges}: {top_pair} -> {idx} {vocab[idx]} has {stats[top_pair]} occurences\")\n",
    "            idx +=1\n",
    "        \n",
    "        self.merges = merges\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Converts ids to a string\"\"\"\n",
    "        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Retums ids from text\"\"\"\n",
    "        ids = list(text.encode('utf-8'))\n",
    "        while len(ids) >= 2:\n",
    "            # find the element in stats that has the smallest associated value in merges\n",
    "            stats = get_stats(ids)\n",
    "            top_pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
    "            if top_pair not in self.merges:\n",
    "                break\n",
    "            ids = merge(ids, top_pair, self.merges[top_pair])\n",
    "        \n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.train(\"How are you doing\", 257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"abcd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/htkumar/llms/tokenization/minbpe',\n",
       " '/Users/htkumar/anaconda3/envs/myenv/lib/python38.zip',\n",
       " '/Users/htkumar/anaconda3/envs/myenv/lib/python3.8',\n",
       " '/Users/htkumar/anaconda3/envs/myenv/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/Users/htkumar/anaconda3/envs/myenv/lib/python3.8/site-packages']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dirname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dirname = os.path.dirname(os.path.abspath(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;18;43m__file__\u001b[39;49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "print(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
