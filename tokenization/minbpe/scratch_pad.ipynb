{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids, counts=None):\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    \n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from unicodedata import category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cc'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\u000a'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch = '\\n'\n",
    "f\"\\\\u{ord(ch):04x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_control_characters(s: str) -> str:\n",
    "    chars = []\n",
    "    for ch in s:\n",
    "        if unicodedata.category(ch)[0] != 'C':\n",
    "            chars.append(ch)\n",
    "        else:\n",
    "            chars.append(f\"\\\\u{ord(ch):04x}\") # escape\n",
    "    \n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd\\\\u000ae\\\\u000d'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_control_characters(\"abcd\\ne\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_token(t: bytes) -> str:\n",
    "    s = t.decode('utf-8', errors='replace')\n",
    "    return replace_control_characters(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'A'\n",
      "b'\\xe2\\x82\\xac'\n",
      "b'\\xf0\\x9f\\x98\\x8a'\n",
      "b'Hello, world!'\n",
      "b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd, world!'\n"
     ]
    }
   ],
   "source": [
    "# Encoding individual characters\n",
    "# Encoding the character 'A' (U+0041)\n",
    "utf8_encoded_A = b'\\x41'\n",
    "print(utf8_encoded_A)  # Output: b'A'\n",
    "\n",
    "# Encoding the Euro sign 'â‚¬' (U+20AC)\n",
    "utf8_encoded_euro = b'\\xe2\\x82\\xac'\n",
    "print(utf8_encoded_euro)  # Output: b'\\xe2\\x82\\xac'\n",
    "\n",
    "# Encoding the emoji 'ðŸ˜Š' (U+1F60A)\n",
    "utf8_encoded_emoji = b'\\xf0\\x9f\\x98\\x8a'\n",
    "print(utf8_encoded_emoji)  # Output: b'\\xf0\\x9f\\x98\\x8a'\n",
    "\n",
    "\n",
    "# Encoding a string\n",
    "# Encoding the string 'Hello, world!' in UTF-8\n",
    "utf8_encoded_string = 'Hello, world!'.encode('utf-8')\n",
    "print(utf8_encoded_string)  # Output: b'Hello, world!'\n",
    "\n",
    "# Encoding a string with characters from multiple scripts\n",
    "# Encoding the string 'ä½ å¥½, world!' containing Chinese characters (U+4F60 U+597D)\n",
    "utf8_encoded_multilingual_string = 'ä½ å¥½, world!'.encode('utf-8')\n",
    "print(utf8_encoded_multilingual_string)  # Output: b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd, world!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[228, 189, 160, 229, 165, 189, 44, 32, 119, 111, 114, 108, 100, 33]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(utf8_encoded_multilingual_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(utf8_encoded_multilingual_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8388608.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.pow(2, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.090339630053647"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(65535)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 13, 13, 240, 159, 152, 138, 194, 174, 226, 130, 172]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understanding utf-8 encoding (smiley is reprsented using 4 bytes, registered using 2 and euro using 3)\n",
    "# each number in the output list is [0, 255]\n",
    "a = '\\n\\r\\rðŸ˜ŠÂ®â‚¬'\n",
    "#  list(b\"\".join([a.encode('utf-8')]))\n",
    "list(a.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\u000a\\\\u000dabcd'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_control_characters('\\n\\rabcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\r\\r\\xf0\\x9f\\x98\\x8a\\xc2\\xae\\xe2\\x82\\xac'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\u000a\\\\u000d\\\\u000dðŸ˜ŠÂ®â‚¬'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_token(a.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "a[(1, 1)] = 2\n",
    "a[(2, 2)] = 3\n",
    "a[(3, 3)] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 2\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "for idx1, idx2 in a:\n",
    "    print(idx1, idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) 2\n",
      "(2, 2) 3\n",
      "(3, 3) 4\n"
     ]
    }
   ],
   "source": [
    "for i,j in a.items():\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('abcd.model')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "p = Path('abcd')\n",
    "model_file = p.with_suffix('.model')\n",
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_file, 'w') as f:\n",
    "    f.write('abcdefg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"Base class for tokenizer\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # default vocab size is 256 (same as ascii chars), no merges, no patterns\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.pattern = \"\" # str\n",
    "        self.special_tokens = {} # str -> int eg. {'<|endoftext|>': 1}\n",
    "        self.vocab = self._build_vocab() # int -> bytes\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        vocab = {idx: bytes(idx) for idx in range(256)}\n",
    "        # the fact that iteration order is same as order in which items are inserted is key here, otherwise we don't have vocab entries for previous merges\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode('utf-8')\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def encode(self, text):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def save(self, file_prefix):\n",
    "        \"\"\"\n",
    "        Saves two files: file_prefix.vocab and file_prefix.model\n",
    "        Similar to sentencepiece\n",
    "        - model file is used for model loading, vocab is just for human viz.\n",
    "        \"\"\"\n",
    "        file = Path(file_prefix)\n",
    "        model_file = file.with_suffix('.model')\n",
    "        with open(model_file, 'w') as f:\n",
    "            # write version, pattern and merges\n",
    "            f.write('minbpe v1\\n')\n",
    "            f.write(f\"{self.pattern}\\n\")\n",
    "            # special tokens\n",
    "            f.write(f\"{len(self.special_tokens)}\\n\")\n",
    "            for special, idx in self.special_tokens.items():\n",
    "                f.write(f\"{special} {idx}\\n\")\n",
    "            \n",
    "            # merges dict\n",
    "            for idx1, idx2 in self.merges: # write only the ids of the merge\n",
    "                f.write(f\"{idx1} {idx2}\\n\")\n",
    "        \n",
    "        # write the vocab, for human viz\n",
    "        # vocab file is different than actual vocab, file is lossy but self.vocab is good.\n",
    "        vocab_file = file.with_suffix('.vocab')\n",
    "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
    "        with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "            for idx, token in self.vocab.items():\n",
    "                # replaces some partial utf-8 seq into ? token, so this can't be decoded due to error = 'replace'\n",
    "                s = render_token(token)\n",
    "                if idx in inverted_merges:\n",
    "                    idx0, idx1 = inverted_merges[idx]\n",
    "                    s0 = render_token(self.vocab[idx0])\n",
    "                    s1 = render_token(self.vocab[idx1])\n",
    "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\") # we should be able to change this\n",
    "                else:\n",
    "                    # print the bytes and special characters, double check the special characters part.\n",
    "                    f.write(f\"[{s}] {idx}\")\n",
    "    \n",
    "    def load(self, model_file):\n",
    "        \"\"\"Invert the functionality in save, but only for model file\"\"\"\n",
    "        assert model_file.endswith(\".model\")\n",
    "        # read the model file\n",
    "        merges = {}\n",
    "        special_tokens = {}\n",
    "        idx = 256\n",
    "\n",
    "        with open(model_file, 'r', encoding='utf-8') as f: # this is decoding, but understand this part more.\n",
    "            version = f.readline().strip()\n",
    "            assert version == \"minbpe v1\"\n",
    "            self.pattern = f.readline().strip()\n",
    "            num_special = int(f.readline().strip())\n",
    "            for _ in range(num_special):\n",
    "                special, special_idx = f.readline().strip().split()\n",
    "                special_tokens[special] = int(special_idx)\n",
    "\n",
    "            # read the merges\n",
    "            for line in f:\n",
    "                idx1, idx2 = map(int, line.split())\n",
    "                merges[(idx1, idx2)] = idx\n",
    "                idx += 1\n",
    "        \n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab = self._build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "        ids = list(text.encode('utf-8'))\n",
    "\n",
    "        merges = {}\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        idx = 256\n",
    "        for i in range(num_merges):\n",
    "            stats = get_stats(ids)\n",
    "            top_pair = max(stats, key=stats.get)\n",
    "            ids = merge(ids, top_pair, idx)\n",
    "            merges[top_pair] = idx\n",
    "            vocab[idx] = vocab[top_pair[0]] + vocab[top_pair[1]]\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{merges}: {top_pair} -> {idx} {vocab[idx]} has {stats[top_pair]} occurences\")\n",
    "            idx +=1\n",
    "        \n",
    "        self.merges = merges\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Converts ids to a string\"\"\"\n",
    "        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Retums ids from text\"\"\"\n",
    "        ids = list(text.encode('utf-8'))\n",
    "        while len(ids) >= 2:\n",
    "            # find the element in stats that has the smallest associated value in merges\n",
    "            stats = get_stats(ids)\n",
    "            top_pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
    "            if top_pair not in self.merges:\n",
    "                break\n",
    "            ids = merge(ids, top_pair, self.merges[top_pair])\n",
    "        \n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.train(\"How are you doing\", 257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"abcd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/htkumar/llms/tokenization/minbpe',\n",
       " '/Users/htkumar/anaconda3/envs/myenv/lib/python38.zip',\n",
       " '/Users/htkumar/anaconda3/envs/myenv/lib/python3.8',\n",
       " '/Users/htkumar/anaconda3/envs/myenv/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/Users/htkumar/anaconda3/envs/myenv/lib/python3.8/site-packages']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# dirname = os.path.dirname(os.path.abspath(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Tokenizer"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_pattern = re.compile(GPT4_SPLIT_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"How are you doing 123's        :) ðŸ˜‰\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', ' are', ' you', ' doing', ' ', '123', \"'s\", '       ', ' :)', ' ðŸ˜‰']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4_tokens = re.findall(compiled_pattern, text)\n",
    "gpt4_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 240, 159, 152, 137]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gpt4_tokens[9].encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[72, 111, 119],\n",
       " [32, 97, 114, 101],\n",
       " [32, 121, 111, 117],\n",
       " [32, 100, 111, 105, 110, 103],\n",
       " [32],\n",
       " [49, 50, 51],\n",
       " [39, 115],\n",
       " [32, 32, 32, 32, 32, 32, 32],\n",
       " [32, 58, 41],\n",
       " [32, 240, 159, 152, 137]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [list(ch.encode('utf-8')) for ch in gpt4_tokens]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges = {} # (int, int) -> int\n",
    "vocab = {idx: bytes(idx) for idx in range(256)}\n",
    "vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes([128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x00\\x00\\x00\\x00\\x00'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(5) # returns bytes object with null bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bytes??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(num_merges):\n",
    "#     # maintain global count of occurences of consecutive pairs\n",
    "#     stats = {}\n",
    "#     for chunk_id in ids:\n",
    "#         get_stats(chunk_id, stats)\n",
    "    \n",
    "#     # find the pair which occurs most consecutively\n",
    "#     max_pair = max(stats, key=stats.get)\n",
    "#     new_id = 256 + i\n",
    "#     ids = [merge(chunk_id, max_pair, new_id) for chunk_id in ids]\n",
    "#     merge[max_pair] = new_id\n",
    "#     vocab[new_id] = vocab[max_pair[0]] + vocab[max_pair[1]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexTokenizer(Tokenizer):\n",
    "    def __init__(self, pattern=None):\n",
    "        \"\"\"\n",
    "        - pattern to split the text by, default is gpt-4 pattern\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        self.special_tokens = {} # dict from str -> int reprsenting special tokens\n",
    "        self.inverse_special_tokens = {} # dict from int -> str reprsenting special tokens\n",
    "    \n",
    "    def register_special_tokens(self, special_tokens):\n",
    "        # special tokens is a dict from str -> int\n",
    "        self.special_tokens = special_tokens\n",
    "        self.inverse_special_tokens = {v:k for k, v in self.special_tokens.items()}\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "        ids = [list(chunk.encode('utf-8')) for chunk in text_chunks]\n",
    "        merges = {}\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            # maintain global count of occurences of consecutive pairs\n",
    "            stats = {}\n",
    "            for chunk_id in ids:\n",
    "                get_stats(chunk_id, stats)\n",
    "            \n",
    "            # find the pair which occurs most consecutively\n",
    "            max_pair = max(stats, key=stats.get)\n",
    "            new_id = 256 + i\n",
    "            ids = [merge(chunk_id, max_pair, new_id) for chunk_id in ids]\n",
    "            merges[max_pair] = new_id\n",
    "            vocab[new_id] = vocab[max_pair[0]] + vocab[max_pair[1]]\n",
    "\n",
    "            # print stats\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {max_pair} -> {new_id} ({vocab[new_id]}) had {stats[max_pair]} occurences\")\n",
    "            \n",
    "        self.merges = merges # used in encode()\n",
    "        self.vocab = vocab # used in decode()\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # return python string given list of integers\n",
    "        part_bytes = []\n",
    "        for idx in ids:\n",
    "            if idx in self.vocab:\n",
    "                part_bytes.append(self.vocab[idx])\n",
    "            elif idx in self.inverse_special_tokens:  # understand this part thoroughly\n",
    "                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid token for decoding: {idx}\")\n",
    "        \n",
    "        s = b\"\".join(part_bytes)\n",
    "        return s.decode('utf-8', errors='replace')\n",
    "    \n",
    "    def _encode_chunk(self, text_bytes):\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >= 2:\n",
    "            # find the element in stats that has the smallest associated value in merges\n",
    "            stats = get_stats(ids)\n",
    "            top_pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
    "            if top_pair not in self.merges:\n",
    "                break\n",
    "            ids = merge(ids, top_pair, self.merges[top_pair])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def _encode_ordinary(self, text):\n",
    "        \"Encoding that ignores any special tokens\"\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "        text_bytes = [chunk.encode('utf-8') for chunk in text_chunks]\n",
    "        encoded_out = []\n",
    "        for text_byte in text_bytes:\n",
    "            encoded_out.extend(self._encode_chunk(text_byte))\n",
    "        \n",
    "        return encoded_out\n",
    "    \n",
    "    def encode(self, text, allowed_special=\"none_raise\"):\n",
    "        \"\"\"\n",
    "        This function handles special tokens\n",
    "        allowed_special: can be \"all\"|\"none\"|\"none_raise\"\n",
    "        tiktoken default behavior is none_raise\n",
    "        \"\"\"\n",
    "        special = None\n",
    "        if allowed_special == \"all\":\n",
    "            special = self.special_tokens\n",
    "        elif allowed_special == \"none\":\n",
    "            special = {}\n",
    "        elif allowed_special == \"none_raise\":\n",
    "            special = {}\n",
    "            assert all(token not in text for token in self.special_tokens)\n",
    "        elif isinstance(allowed_special, set):\n",
    "            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n",
    "        else:\n",
    "            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
    "        \n",
    "        if not special:\n",
    "            # revert to encode ordinary\n",
    "            return self._encode_ordinary(text)\n",
    "        \n",
    "        # else need to handle special characters\n",
    "        # enclosing in parenthesis makes it into a capturing group so that special tokens are includes in the output from split \n",
    "        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "        special_chunks = re.split(special_pattern, text)\n",
    "        ids = []\n",
    "        for chunk in special_chunks:\n",
    "            if chunk in special:\n",
    "                ids.append(special[chunk])\n",
    "            else:\n",
    "                ids.extend(self._encode_ordinary(chunk))\n",
    "        \n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps\n",
    "# understanding gpt-2 and gpt-4 patterns more closely.\n",
    "# Understand the chunking behavior of gpt4 and why it needs lists of integers.\n",
    "# understand the special characters usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2], [3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = []\n",
    "for i in a:\n",
    "    b.extend(i)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "special = {\n",
    "    '<|endoftext|>': 100257,\n",
    "    '<|endofprompt|>': 100258,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<\\\\|endoftext\\\\|>|<\\\\|endofprompt\\\\|>)'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "special_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.escape('<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(0, 13), match='<|endoftext|>'>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(p, '<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(0, 1), match='<'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('<|endoftext|>', '<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"def <|endoftext|>Last document!!! ðŸ‘‹<|endofprompt|> abcd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def ', '<|endoftext|>', 'Last document!!! ðŸ‘‹', '<|endofprompt|>', ' abcd']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_chunks = re.split(special_pattern, text)\n",
    "special_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/4: (72, 111) -> 256 (b'Ho') had 2 occurences\n",
      "merge 2/4: (256, 119) -> 257 (b'How') had 2 occurences\n",
      "merge 3/4: (32, 97) -> 258 (b' a') had 2 occurences\n",
      "merge 4/4: (258, 114) -> 259 (b' ar') had 2 occurences\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "long_str = \"How are you doing, How are you doing\"\n",
    "tokenizer.train(long_str, verbose=True, vocab_size=260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Hello World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
