{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 490226347214825,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from model import GPT, GPTConfig, MoeArgs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 864599185510497,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "experiment_id = \"base_gpt2\"\n",
    "LOGS_DIR = f\"/home/htkumar/llms/gpt2_karpathy/logs_{experiment_id}\"\n",
    "num_epochs = 1\n",
    "step = 19073 * num_epochs - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 1042965680396393,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "checkpoint_file = torch.load(os.path.join(LOGS_DIR, f\"model_{step:05d}.pt\"))\n",
    "model.load_state_dict(checkpoint_file[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_model(model):\n",
    "    # generate from the model\n",
    "    num_return_sequences = 5\n",
    "    max_length = 32\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "    tokens = [15496, 11, 314, 1101, 257, 3303, 2746, 11]\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)  # (8,)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)  # (5, 8)\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(42)\n",
    "\n",
    "    # (B, T)\n",
    "    while x.size(1) < max_length:\n",
    "        with torch.no_grad():\n",
    "            # (B, T, vocab_size)\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x)\n",
    "            # (B, vocab_size)\n",
    "            # print(logits.shape)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # (B, 50)\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "            new_id = torch.multinomial(topk_probs, num_samples=1)  # (B, 1)\n",
    "            new_id = torch.gather(topk_indices, -1, new_id)  # (B, 1)\n",
    "            # (B, T + 1)\n",
    "            x = torch.cat((x, new_id), dim=-1)\n",
    "\n",
    "    for i in range(num_return_sequences):\n",
    "        decoded = enc.decode(x[i].tolist())\n",
    "        print(f\"{i} {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_hf_model(model):\n",
    "    # generate from the model\n",
    "    num_return_sequences = 5\n",
    "    max_length = 32\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "    tokens = [15496, 11, 314, 1101, 257, 3303, 2746, 11]\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)  # (8,)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)  # (5, 8)\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(42)\n",
    "\n",
    "    # (B, T)\n",
    "    while x.size(1) < max_length:\n",
    "        with torch.no_grad():\n",
    "            # (B, T, vocab_size)\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits = model(x)[0]\n",
    "            # (B, vocab_size)\n",
    "            # print(logits.shape)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # (B, 50)\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "            new_id = torch.multinomial(topk_probs, num_samples=1)  # (B, 1)\n",
    "            new_id = torch.gather(topk_indices, -1, new_id)  # (B, 1)\n",
    "            # (B, T + 1)\n",
    "            x = torch.cat((x, new_id), dim=-1)\n",
    "\n",
    "    for i in range(num_return_sequences):\n",
    "        decoded = enc.decode(x[i].tolist())\n",
    "        print(f\"{i} {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 831422379197368,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "sd_hf = model_hf.state_dict()\n",
    "sd_hf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf.eval()\n",
    "model_hf.to(device)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "generate_from_hf_model(model_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sd_hf[\"lm_head.weight\"].data_ptr())\n",
    "print(sd_hf[\"transformer.wte.weight\"].data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_hf[\"transformer.wpe.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_hf[\"transformer.wpe.weight\"].view(-1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sd_hf[\"transformer.wpe.weight\"], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300, :300], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline, set_seed\n",
    "\n",
    "# generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "# set_seed(42)\n",
    "# generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check impact of gradient accumulation\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(16, 32),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(32, 1),\n",
    ")\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "x = torch.randn(4, 16)\n",
    "y = torch.randn(4, 1)\n",
    "net.zero_grad()\n",
    "yhat = net(x)\n",
    "\n",
    "loss = F.mse_loss(yhat, y)\n",
    "loss.backward()\n",
    "net[0].weight.grad.view(-1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "for i in range(4):\n",
    "    yhat = net(x[i])\n",
    "    loss = F.mse_loss(yhat, y[i])\n",
    "    loss /= 4\n",
    "    loss.backward()\n",
    "\n",
    "net[0].weight.grad.view(-1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the logfile generated after initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate validation loss of gpt2 from hf to serve as baseline\n",
    "from dataloader import DataLoaderLite\n",
    "\n",
    "val_dataloader = DataLoaderLite(\n",
    "    B=16, T=2048, process_rank=0, num_processes=1, split=\"val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate gpt-2 baseline model\n",
    "val_dataloader.reset()\n",
    "with torch.no_grad():\n",
    "    val_loss_accum = 0.0\n",
    "    val_loss_steps = 100\n",
    "    loss_total = 0.0\n",
    "    for _ in tqdm(range(val_loss_steps), desc=\"Evaluating validation loss\"):\n",
    "        x, y = val_dataloader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model_hf(x)[0]\n",
    "        # print(logits.size()[-1])\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size()[-1]), y.view(-1))\n",
    "        loss_total += loss.detach()\n",
    "\n",
    "    loss_avg = loss_total / val_loss_steps\n",
    "    print(loss_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader.reset()\n",
    "with torch.no_grad():\n",
    "    val_loss_accum = 0.0\n",
    "    val_loss_steps = 100\n",
    "    loss_total = 0.0\n",
    "    for _ in tqdm(range(val_loss_steps), desc=\"Evaluating validation loss\"):\n",
    "        x, y = val_dataloader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        # print(logits.size()[-1])\n",
    "        # loss = F.cross_entropy(logits.view(-1, logits.size()[-1]), y.view(-1))\n",
    "        loss_total += loss.detach()\n",
    "\n",
    "    loss_avg = loss_total / val_loss_steps\n",
    "    print(loss_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sz = \"124M\"\n",
    "loss_baseline = {\n",
    "    \"124M\": 3.2758,\n",
    "}[sz]\n",
    "\n",
    "with open(f\"{LOGS_DIR}/log.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = {}\n",
    "for line in lines:\n",
    "    step, stream, val = line.strip().split()\n",
    "    if stream not in streams:\n",
    "        streams[stream] = {}\n",
    "    streams[stream][int(step)] = float(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(streams.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams_xy = {}\n",
    "for k, v in streams.items():\n",
    "    xy = sorted(list(v.items()))\n",
    "    # print(xy)\n",
    "    # print(zip(*xy))\n",
    "    streams_xy[k] = list(zip(*xy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = streams_xy[\"train\"]\n",
    "ys = np.array(ys)\n",
    "print(f\"min train loss {min(ys)}\")\n",
    "\n",
    "xs_val, ys_val = streams_xy[\"val\"]\n",
    "ys_val = np.array(ys_val)\n",
    "print(f\"min val loss {min(ys_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, ys, label=f\"nanogpt {sz} train loss\")\n",
    "plt.plot(xs_val, ys_val, label=f\"nanogpt {sz} val loss\")\n",
    "\n",
    "if loss_baseline is not None:\n",
    "    plt.axhline(\n",
    "        y=loss_baseline, color=\"r\", linestyle=\"--\", label=f\"OpenAI gpt-2 {sz} model\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(top=4.0)\n",
    "plt.legend()\n",
    "plt.title(\"Loss curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(model_hf.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 521M for MOE based model, 124M for non-moe based model\n",
    "get_num_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1024 * 768  # extra pos embeddings brought by increasing sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scratch pad below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(1, 2), (2, 3), (3, 4)]\n",
    "b = list(zip(*a))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "b = [2, 3, 4, 5]\n",
    "for i, j in zip(a, b):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
