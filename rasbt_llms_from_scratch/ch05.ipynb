{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from importlib.metadata import version\n",
        "pkgs = [\n",
        "    \"matplotlib\",\n",
        "    \"numpy\",\n",
        "    \"tiktoken\"\n",
        "]\n",
        "for pkg in pkgs:\n",
        "    print(f\"{pkg}: {version(pkg)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_download import BASE_CONFIG, model_configs\n",
        "from gpt_model import GPTModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = BASE_CONFIG.copy()\n",
        "config.update(model_configs[\"gpt2-small (124M)\"])\n",
        "model = GPTModel(config)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from gpt_model import generate_text_simple, token_ids_to_text, text_to_token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "ids = tokenizer.encode(\"How are you doing today?\")\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "token_ids = generate_text_simple(\n",
        "    model,\n",
        "    text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    20,\n",
        "    context_size=config['context_length']\n",
        ")\n",
        "token_ids_to_text(token_ids, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts = [\n",
        "    \"every effort moves you\",\n",
        "    \"I really like chocolate\",\n",
        "]\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for text in texts:\n",
        "    ids = text_to_token_ids(text, tokenizer).squeeze(0)\n",
        "    inputs.append(ids[:-1])\n",
        "    targets.append(ids[1:])\n",
        "\n",
        "inputs = torch.stack(inputs, dim=0)\n",
        "targets = torch.stack(targets, dim=0)\n",
        "print(inputs)\n",
        "print(targets, targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    logits = model(inputs)\n",
        "\n",
        "probas = torch.softmax(logits, dim=-1)\n",
        "probas.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_tokens = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "predicted_tokens.shape\n",
        "predicted_tokens[0].flatten().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "token_ids_to_text(targets[0], tokenizer)\n",
        "targets[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "token_ids_to_text(predicted_tokens[0].flatten(), tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "probas.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "probs = []\n",
        "for text_idx in [0, 1]:\n",
        "    probs.append(probas[text_idx, [0, 1, 2], targets[text_idx]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "probs_1 = torch.log(torch.cat(probs, dim=0))\n",
        "probs_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg_log_probs = torch.mean(probs_1)\n",
        "print(avg_log_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net_avg_log_probas = avg_log_probs * -1\n",
        "print(net_avg_log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss = F.cross_entropy(logits.view(-1, 50257), targets.flatten())\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perplexity = torch.exp(loss)\n",
        "perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = \"/home/htkumar/llms/rasbt_llms_from_scratch/the-verdict.txt\"\n",
        "# url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "# # Download the dataset if it doesn't already exist\n",
        "# if not os.path.exists(file_path):\n",
        "#     response = requests.get(url)\n",
        "#     with open(file_path, 'wb') as f:\n",
        "#         f.write(response.content)\n",
        "\n",
        "# Read the dataset\n",
        "with open(file_path, 'r') as f:\n",
        "    text_data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(text_data[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(text_data), len(tokenizer.encode(text_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_dataloader import GPTDatasetV1, create_dataloader_v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train/validation ratio\n",
        "\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=256,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=256,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = GPTDatasetV1(train_data, tokenizer)\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "    print(x.numel())\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x.size(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_batch)\n",
        "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_batch.flatten())\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_model import calc_loss_loader, train_model_simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_loss = calc_loss_loader(train_loader, model, device)\n",
        "val_loss = calc_loss_loader(val_loader, model, device)\n",
        "print(train_loss, val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GPTModel(config)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer, max_new_tokens=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "epochs_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_model import plot_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "idx = text_to_token_ids(\"Every effort moves you\", tokenizer).to(device)\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=idx,\n",
        "    max_new_tokens=25,\n",
        "    context_size=config['context_length']\n",
        ")\n",
        "\n",
        "print(f\"Output text is {token_ids_to_text(token_ids, tokenizer)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "inverse_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "next_token_logits = torch.tensor(\n",
        "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")\n",
        "next_token_probs = torch.softmax(next_token_logits, dim=0)\n",
        "next_token = torch.multinomial(next_token_probs, 1)\n",
        "next_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.bincount??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.tensor([0, 0, 0, 2, 2, 0, 1])\n",
        "torch.bincount(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_sampled_tokens(probas):\n",
        "    torch.manual_seed(123)\n",
        "    samples = [torch.multinomial(probas, 1).item() for i in range(1000)]\n",
        "    counts = torch.bincount(torch.tensor(samples))\n",
        "    for i, c in enumerate(counts):\n",
        "        print(f\"{i} ... {c} {inverse_vocab[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_sampled_tokens(next_token_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax_with_temperature(logits, temperature):\n",
        "    return torch.softmax(logits / temperature, dim=-1)\n",
        "\n",
        "temperatures = [1, 0.1, 5, 10]\n",
        "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
        "scaled_probas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plotting\n",
        "x = torch.arange(len(vocab))\n",
        "bar_width = 0.15\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 3))\n",
        "for i, t in enumerate(temperatures):\n",
        "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f\"Temperature = {t}\")\n",
        "\n",
        "ax.set_ylabel(\"Probability\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_sampled_tokens(scaled_probas[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_sampled_tokens(scaled_probas[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_sampled_tokens(scaled_probas[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "next_token_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(\n",
        "    model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None\n",
        "):\n",
        "\n",
        "    # TODO: Verify that this generates the same outputs as in master repo\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # shape is (B, V)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        if top_k is not None:\n",
        "            topk_probs, _ = torch.topk(logits, top_k)\n",
        "            min_val = topk_probs[:, -1]\n",
        "            logits = torch.where(\n",
        "                logits < min_val,\n",
        "                torch.tensor(-float(\"inf\")).to(logits.device),\n",
        "                logits,\n",
        "            )\n",
        "\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (B, T + 1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=idx,\n",
        "    max_new_tokens=25,\n",
        "    context_size=config['context_length'],\n",
        "    temperature=5,\n",
        "    top_k=15,\n",
        ")\n",
        "\n",
        "print(f\"Output text is {token_ids_to_text(token_ids, tokenizer)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.state_dict().keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dir = \"/home/htkumar/llms/rasbt_llms_from_scratch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict()\n",
        "    },\n",
        "    f\"{model_dir}/model_and_optimizer.pth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = torch.load(f\"{model_dir}/model_and_optimizer.pth\", weights_only=True)\n",
        "model_new = GPTModel(config)\n",
        "model_new.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "optimizer = torch.optim.AdamW(model_new.parameters(), lr=0.0005, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "model_new.train();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "model_new.to(device)\n",
        "token_ids = generate(\n",
        "    model=model_new,\n",
        "    idx=idx,\n",
        "    max_new_tokens=25,\n",
        "    context_size=config['context_length'],\n",
        "    temperature=5,\n",
        "    top_k=15,\n",
        ")\n",
        "\n",
        "print(f\"Output text is {token_ids_to_text(token_ids, tokenizer)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_download import download_and_load_gpt2, load_weights_into_gpt, load_gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "settings, params = load_gpt2(f\"{model_dir}/gpt2/124M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt = GPTModel(config)\n",
        "gpt.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "idx = text_to_token_ids(\"Every effort moves you\", tokenizer).to(device)\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=idx,\n",
        "    max_new_tokens=50,\n",
        "    context_size=config['context_length'],\n",
        "    temperature=5,\n",
        "    top_k=15,\n",
        ")\n",
        "\n",
        "print(f\"Output text is {token_ids_to_text(token_ids, tokenizer)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "idx = text_to_token_ids(\"Every effort moves you\", tokenizer).to(device)\n",
        "token_ids = generate(\n",
        "    model=model_new,\n",
        "    idx=idx,\n",
        "    max_new_tokens=50,\n",
        "    context_size=config['context_length'],\n",
        "    temperature=5,\n",
        "    top_k=15,\n",
        ")\n",
        "\n",
        "print(f\"Output text is {token_ids_to_text(token_ids, tokenizer)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "a05a1ae6-8597-4a40-91e6-c6901cf955d1",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "gpt-dev (local)",
      "language": "python",
      "name": "gpt-dev_local"
    },
    "language_info": {
      "name": "plaintext"
    }
  }
}
