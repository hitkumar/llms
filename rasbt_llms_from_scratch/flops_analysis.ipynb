{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\n",
        "    'torch',\n",
        "    'thop',\n",
        "]\n",
        "for p in pkgs:\n",
        "    print(f\"{p}: {version(p)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from thop import profile\n",
        "from gpt_model import GPTModel\n",
        "from gpt_download import BASE_CONFIG, model_configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 2\n",
        "input_tensor = torch.randint(0, 50257, (batch_size, 1024)).to(device)\n",
        "input_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = BASE_CONFIG.copy()\n",
        "for size in model_configs:\n",
        "    config.update(model_configs[size])\n",
        "    print(config)\n",
        "    model = GPTModel(config).bfloat16()\n",
        "    model.to(device)\n",
        "\n",
        "    macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
        "    flops = macs * 2\n",
        "    print(f\"{size:18} flops = {flops:.1e}, number of parameters: {params/1e6:.2f}M\")\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# start_time = time.time()\n",
        "config = BASE_CONFIG.copy()\n",
        "for size in model_configs:\n",
        "    min_batch_size = 1\n",
        "    max_batch_size = None\n",
        "    max_possible_batch_size = 4096\n",
        "\n",
        "    config.update(model_configs[size])\n",
        "\n",
        "    while min_batch_size < max_possible_batch_size:\n",
        "        batch_size = (min_batch_size + max_possible_batch_size) // 2\n",
        "        try:\n",
        "            input_tensor = torch.randint(0, 50257, (batch_size, 1024)).to(device)\n",
        "            model = GPTModel(config).bfloat16()\n",
        "            model.to(device)\n",
        "            macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
        "            flops = macs * 2\n",
        "            print(f\"{size:18} flops = {flops:.1e}, number of parameters: {params/1e6:.2f}M, batch_size = {batch_size}\")\n",
        "\n",
        "            min_batch_size = batch_size + 1\n",
        "            max_batch_size = batch_size\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                max_possible_batch_size = batch_size - 1\n",
        "\n",
        "            del model, input_tensor\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"{size:18} flops = {flops:.1e}, number of parameters: {params/1e6:.2f}M, max batch_size = {max_batch_size}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_flops_per_second = {\n",
        "    \"A100\": {\n",
        "        torch.float32: 19.49e12,\n",
        "        torch.float16: 77.97e12,\n",
        "        torch.bfloat16: 77.97e12,\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device_name = torch.cuda.get_device_name(0)\n",
        "for model_name in max_flops_per_second:\n",
        "    if model_name in device_name:\n",
        "        print(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = GPTModel(config).bfloat16()\n",
        "# model.to(device)\n",
        "# data_type = next(model.parameters()).dtype\n",
        "# print(data_type)\n",
        "# max_flops_per_second = max_flops_per_second[model_name].get(data_type, 0)\n",
        "\n",
        "# del model\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = BASE_CONFIG.copy()\n",
        "model_name = 'A100'\n",
        "for size in model_configs:\n",
        "    min_batch_size = 1\n",
        "    max_batch_size = None\n",
        "    max_possible_batch_size = 4096\n",
        "\n",
        "    config.update(model_configs[size])\n",
        "\n",
        "    while min_batch_size < max_possible_batch_size:\n",
        "        batch_size = (min_batch_size + max_possible_batch_size) // 2\n",
        "        try:\n",
        "            input_tensor = torch.randint(0, 50257, (batch_size, 1024)).to(device)\n",
        "            model = GPTModel(config).bfloat16()\n",
        "            model.to(device)\n",
        "            model.train()\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            start_time = time.time()\n",
        "            output = model(input_tensor)\n",
        "            loss = output.sum()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "\n",
        "            macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
        "            flops_forward = macs * 2\n",
        "            flops_backward = flops_forward * 2\n",
        "            total_flops = flops_forward + flops_backward\n",
        "\n",
        "            data_type = next(model.parameters()).dtype\n",
        "            max_flops_per_second_model = max_flops_per_second[model_name].get(data_type, 0)\n",
        "            tokens_processed = batch_size * 1024\n",
        "            observed_tokens_per_sec = tokens_processed / elapsed_time\n",
        "\n",
        "            theoretical_tokens_per_sec = max_flops_per_second_model / (total_flops / tokens_processed)\n",
        "\n",
        "            mfu = observed_tokens_per_sec / theoretical_tokens_per_sec\n",
        "\n",
        "            print(f\"{size:18} flops = {total_flops:.1e}, number of parameters: {params/1e6:.2f}M, batch_size = {batch_size}, mfu: {mfu:.4f}\")\n",
        "\n",
        "            min_batch_size = batch_size + 1\n",
        "            max_batch_size = batch_size\n",
        "\n",
        "            del model, input_tensor, output, loss\n",
        "            torch.cuda.empty_cache()\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                max_possible_batch_size = batch_size - 1\n",
        "\n",
        "            del model, input_tensor\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"{size:18} flops = {flops:.1e}, number of parameters: {params/1e6:.2f}M, max batch_size = {max_batch_size}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "47181d94-6578-4e0e-b1aa-d925882c958d",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "gpt-dev (local)",
      "language": "python",
      "name": "gpt-dev_local"
    },
    "language_info": {
      "name": "plaintext"
    }
  }
}
