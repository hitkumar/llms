{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"tiktoken\",\n",
    "    \"torch\",\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"/home/htkumar/llms/rasbt_llms_from_scratch/instruction-data-with-preference.json\"\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pp(data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pp(data[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pp(data[900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that approximately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else \"\"\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = format_input(data[50])\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_response = f\"### Response: \\n{data[50]['chosen']}\"\n",
    "print(desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_response = f\"### Response: \\n{data[50]['rejected']}\"\n",
    "print(possible_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = lambda entry: f\"### Response: \\n{entry['chosen']}\"\n",
    "print(response_format(data[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion: train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(test_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            prompt = format_input(entry)\n",
    "            rejected_response = entry['rejected']\n",
    "            chosen_response = entry['chosen']\n",
    "            chosen_full_text = f\"{prompt}\\n\\n###Response:\\n{chosen_response}\"\n",
    "            rejected_full_text = f\"{prompt}\\n\\n###Response:\\n{rejected_response}\"\n",
    "\n",
    "            prompt_tokens = tokenizer.encode(prompt)\n",
    "            chosen_full_tokens = tokenizer.encode(chosen_full_text)\n",
    "            rejected_full_tokens = tokenizer.encode(rejected_full_text)\n",
    "\n",
    "            self.encoded_texts.append({\n",
    "                'prompt': prompt_tokens,\n",
    "                'chosen': chosen_full_tokens,\n",
    "                'rejected': rejected_full_tokens,\n",
    "            })\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones([10]); b = torch.zeros([10])\n",
    "c = [a, b]\n",
    "d = torch.stack(c); d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        allowed_max_length=None,\n",
    "        mask_prompt_tokens=True,\n",
    "        device='cpu'\n",
    "):\n",
    "    batch_data = {\n",
    "        'prompt': [],\n",
    "        'chosen': [],\n",
    "        'rejected': [],\n",
    "        'rejected_mask': [],\n",
    "        'chosen_mask': []\n",
    "    }\n",
    "\n",
    "    max_length_common = 0\n",
    "    if batch:\n",
    "        for key in ['chosen', 'rejected']:\n",
    "            # why adding +1 here? possibly end of sentence token\n",
    "            current_max = max(len(item[key]) + 1 for item in batch)\n",
    "            max_length_common = max(max_length_common, current_max)\n",
    "\n",
    "    for item in batch:\n",
    "        prompt = torch.tensor(item['prompt'])\n",
    "        batch_data['prompt'].append(prompt)\n",
    "        for key in ['chosen', 'rejected']:\n",
    "            sequence = item[key]\n",
    "            padded = sequence + [pad_token_id] * (max_length_common - len(sequence))\n",
    "            mask = torch.ones(len(padded)).bool()\n",
    "\n",
    "            # set mask for padding tokens to be False\n",
    "            mask[len(sequence):] = False\n",
    "\n",
    "            # +2 sets the new 2 newline tokens before ### Response to False\n",
    "            # Set mask for input tokens to be False\n",
    "            if mask_prompt_tokens:\n",
    "                mask[:prompt.shape[0]+2] = False\n",
    "\n",
    "            batch_data[key].append(torch.tensor(padded))\n",
    "            batch_data[f\"{key}_mask\"].append(mask)\n",
    "\n",
    "    # Process batch data\n",
    "    for key in ['chosen', 'rejected', 'chosen_mask', 'rejected_mask']:\n",
    "        # [B, max_length_common]\n",
    "        tensor_stack = torch.stack(batch_data[key])\n",
    "        if allowed_max_length is not None:\n",
    "            tensor_stack = tensor_stack[:, :allowed_max_length]\n",
    "\n",
    "        batch_data[key] = tensor_stack.to(device)\n",
    "\n",
    "    return batch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    mask_prompt_tokens=True,\n",
    "    allowed_max_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = data[:2]\n",
    "for i in example_data:\n",
    "    pprint.pp(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from torch.utils.data import DataLoader\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "example_dataset = PreferenceDataset(example_data, tokenizer)\n",
    "example_dataloader = DataLoader(\n",
    "    example_dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(example_dataloader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['prompt'][0].shape, batch['prompt'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['chosen'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens_from_batch(token_ids, tokenizer):\n",
    "    ids = token_ids.flatten().tolist()\n",
    "    return tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch['prompt'][0],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch['rejected'][0],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['prompt'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['chosen_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch['rejected'][0][batch['rejected_mask'][0]],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch['chosen'][0][batch['chosen_mask'][0]],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mask is used to ignore prompt and padding tokens while computing DPO loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_dataset = PreferenceDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = PreferenceDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PreferenceDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch['chosen'].shape, batch['rejected'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load instruction finetuned model\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "finetuned_model_path = Path('/home/htkumar/llms/rasbt_llms_from_scratch/gpt2-medium-sft.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import GPTModel, generate, text_to_token_ids, token_ids_to_text\n",
    "from gpt_download import load_gpt2, BASE_CONFIG, model_configs\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "model = GPTModel(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        '/home/htkumar/llms/rasbt_llms_from_scratch/gpt2-medium-sft.pth',\n",
    "        map_location=device,\n",
    "        weights_only=True\n",
    "    )\n",
    ")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = format_input(data[2])\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG['context_length'],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = model\n",
    "reference_model = GPTModel(BASE_CONFIG)\n",
    "reference_model.load_state_dict(\n",
    "    torch.load(\n",
    "        '/home/htkumar/llms/rasbt_llms_from_scratch/gpt2-medium-sft.pth',\n",
    "        map_location=device,\n",
    "        weights_only=True\n",
    "    )\n",
    ")\n",
    "reference_model.eval();\n",
    "policy_model.to(device)\n",
    "reference_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DPO loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_dpo_loss(\n",
    "    model_chosen_logprobs,\n",
    "    model_rejected_logprobs,\n",
    "    reference_chosen_logprobs,\n",
    "    reference_rejected_logprobs,\n",
    "    beta=0.1\n",
    "):\n",
    "    model_log_ratios = model_chosen_logprobs - model_rejected_logprobs\n",
    "    reference_log_ratios = reference_chosen_logprobs - reference_rejected_logprobs\n",
    "    logits = model_log_ratios - reference_log_ratios\n",
    "\n",
    "    losses = -F.logsigmoid(beta * logits)\n",
    "    chosen_rewards = (model_chosen_logprobs - reference_chosen_logprobs).detach()\n",
    "    rejected_rewards = (model_rejected_logprobs - reference_rejected_logprobs).detach()\n",
    "\n",
    "    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpo_loss_alt(\n",
    "    model_chosen_logprobs,\n",
    "    model_rejected_logprobs,\n",
    "    reference_chosen_logprobs,\n",
    "    reference_rejected_logprobs,\n",
    "    beta=0.1\n",
    "):\n",
    "    chosen_logprobs = model_chosen_logprobs - reference_chosen_logprobs\n",
    "    rejected_logprobs = model_rejected_logprobs - reference_rejected_logprobs\n",
    "\n",
    "    logits = chosen_logprobs - rejected_logprobs\n",
    "\n",
    "    losses = -F.logsigmoid(beta * logits)\n",
    "    chosen_rewards = (model_chosen_logprobs - reference_chosen_logprobs).detach()\n",
    "    rejected_rewards = (model_rejected_logprobs - reference_rejected_logprobs).detach()\n",
    "\n",
    "    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1., 2., 3.])\n",
    "torch.log(F.softmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.log_softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "logits = torch.tensor(\n",
    "    [[2.0, 1.0, 0.1],\n",
    "    [0.5, 2.5, 0.3]]\n",
    ")\n",
    "targets = torch.tensor([0, 2])\n",
    "logits.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax_logits = F.log_softmax(logits, dim=1)\n",
    "selected_log_probs = torch.gather(\n",
    "    input=log_softmax_logits,\n",
    "    dim=1,\n",
    "    index=targets.unsqueeze(1)\n",
    ").squeeze(1)\n",
    "print(log_softmax_logits)\n",
    "print(selected_log_probs)\n",
    "print(selected_log_probs.shape)\n",
    "manual_loss = -selected_log_probs.mean()\n",
    "print(manual_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = F.cross_entropy(logits, targets)\n",
    "print(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(\n",
    "  [[1., 2.,],\n",
    "   [3., 4.]]\n",
    ")\n",
    "m = torch.tensor(\n",
    "    [[1, 1, 1],\n",
    "    [0, 1, 1]]\n",
    ")\n",
    "\n",
    "selected_nums = torch.gather(\n",
    "    input=t,\n",
    "    dim=1,\n",
    "    index=m\n",
    ")\n",
    "selected_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = torch.tensor([\n",
    "    [0.5, 0.3, 0.2],\n",
    "    [1.0, 2.0, 3.0]\n",
    "])\n",
    "print(log_probs.mean(-1, keepdim=True))\n",
    "mask = torch.tensor([\n",
    "    [False, True, True],\n",
    "    [False, True, True]\n",
    "])\n",
    "log_probs = log_probs * mask\n",
    "(log_probs.sum(-1) / mask.sum(-1)).shape\n",
    "log_probs.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy is the minus of mean of log_probs of the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprobs(logits, labels, selection_mask=None):\n",
    "    \"\"\"\n",
    "    logits is [B, num_tokens, vocab_size]\n",
    "    labels is [B, num_tokens]\n",
    "    selection_mask is [B, num_tokens]\n",
    "    \"\"\"\n",
    "    logits = logits[:, :-1, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    labels = labels[:, 1:]\n",
    "\n",
    "    # shape is [B, num_tokens-1] consisting of log_probs at every index.\n",
    "    selected_log_probs = torch.gather(\n",
    "        input=log_probs,\n",
    "        dim=-1,\n",
    "        index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    if selection_mask is not None:\n",
    "        mask = selection_mask[:, 1:].clone()\n",
    "        # Apply the mask to filter out padding tokens\n",
    "        selected_log_probs = selected_log_probs * mask\n",
    "        avg_log_prob = selected_log_probs.sum(-1) / mask.sum(-1)\n",
    "        return avg_log_prob\n",
    "\n",
    "    return selected_log_probs.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpo_loss_batch(batch, policy_model, reference_model, beta):\n",
    "    policy_model_chosen_logprobs = compute_logprobs(\n",
    "        policy_model(batch['chosen']),\n",
    "        batch['chosen'],\n",
    "        batch['chosen_mask']\n",
    "    )\n",
    "    policy_model_rejected_logprobs = compute_logprobs(\n",
    "        policy_model(batch['rejected']),\n",
    "        batch['rejected'],\n",
    "        batch['rejected_mask']\n",
    "    )\n",
    "    reference_model_chosen_logprobs = compute_logprobs(\n",
    "        reference_model(batch['chosen']),\n",
    "        batch['chosen'],\n",
    "        batch['chosen_mask']\n",
    "    )\n",
    "    reference_model_rejected_logprobs = compute_logprobs(\n",
    "        reference_model(batch['rejected']),\n",
    "        batch['rejected'],\n",
    "        batch['rejected_mask']\n",
    "    )\n",
    "    return compute_dpo_loss(\n",
    "        policy_model_chosen_logprobs,\n",
    "        policy_model_rejected_logprobs,\n",
    "        reference_model_chosen_logprobs,\n",
    "        reference_model_rejected_logprobs,\n",
    "        beta\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dpo_loss_batch(test_batch, policy_model, reference_model, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpo_loss_loader(data_loader, policy_model, reference_model, beta, num_batches=None):\n",
    "    total_loss, total_chosen_rewards, total_rejected_rewards = 0., 0., 0.\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n",
    "                batch,\n",
    "                policy_model,\n",
    "                reference_model,\n",
    "                beta\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            total_chosen_rewards += chosen_rewards.item()\n",
    "            total_rejected_rewards += rejected_rewards.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches, total_chosen_rewards / num_batches, total_rejected_rewards / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dpo_loss_loader(policy_model, reference_model, train_loader, val_loader, beta, eval_iter):\n",
    "    # reference model has always been in eval model since creation.\n",
    "    policy_model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_chosen_rewards, train_rejected_rewards = compute_dpo_loss_loader(\n",
    "            data_loader=train_loader,\n",
    "            policy_model=policy_model,\n",
    "            reference_model=reference_model,\n",
    "            beta=beta,\n",
    "            num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_loss, val_chosen_rewards, val_rejected_rewards = compute_dpo_loss_loader(\n",
    "            data_loader=val_loader,\n",
    "            policy_model=policy_model,\n",
    "            reference_model=reference_model,\n",
    "            beta=beta,\n",
    "            num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "    res = {\n",
    "        'train_loss': train_loss,\n",
    "        'train_chosen_reward': train_chosen_rewards,\n",
    "        'train_rejected_reward': train_rejected_rewards,\n",
    "        'val_loss': val_loss,\n",
    "        'val_chosen_reward': val_chosen_rewards,\n",
    "        'val_rejected_reward': val_rejected_rewards\n",
    "    }\n",
    "    policy_model.train()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import generate_and_print_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_output(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    data,\n",
    "    device='cpu'\n",
    "):\n",
    "    input_text = format_input(data)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=policy_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG['context_length'],\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {data['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text}\")\n",
    "    print(\"\\n----------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_output(policy_model, tokenizer, val_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_dpo_simple(\n",
    "    policy_model,\n",
    "    reference_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    beta,\n",
    "    eval_freq,\n",
    "    eval_iter,\n",
    "    start_context,\n",
    "    tokenizer\n",
    "):\n",
    "    tracking = {\n",
    "        'train_losses': [],\n",
    "        'train_chosen_rewards': [],\n",
    "        'train_rejected_rewards': [],\n",
    "        'val_losses': [],\n",
    "        'val_chosen_rewards': [],\n",
    "        'val_rejected_rewards': [],\n",
    "        'tokens_seen': []\n",
    "    }\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        policy_model.train()\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n",
    "                batch=batch,\n",
    "                policy_model=policy_model,\n",
    "                reference_model=reference_model,\n",
    "                beta=beta\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step() # update model parameters\n",
    "\n",
    "            tokens_seen += batch['chosen'].numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                res = evaluate_dpo_loss_loader(\n",
    "                    policy_model=policy_model,\n",
    "                    reference_model=reference_model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    beta=beta,\n",
    "                    eval_iter=eval_iter\n",
    "                )\n",
    "\n",
    "                tracking['train_losses'].append(res['train_loss'])\n",
    "                tracking[\"train_chosen_rewards\"].append(res[\"train_chosen_reward\"])\n",
    "                tracking[\"train_rejected_rewards\"].append(res[\"train_rejected_reward\"])\n",
    "                tracking[\"val_losses\"].append(res[\"val_loss\"])\n",
    "                tracking[\"val_chosen_rewards\"].append(res[\"val_chosen_reward\"])\n",
    "                tracking[\"val_rejected_rewards\"].append(res[\"val_rejected_reward\"])\n",
    "                tracking[\"tokens_seen\"].append(tokens_seen)\n",
    "\n",
    "                train_reward_margin = res['train_chosen_reward'] - res['train_rejected_reward']\n",
    "                val_reward_margin = res['val_chosen_reward'] - res['val_rejected_reward']\n",
    "\n",
    "                print(\n",
    "                    f\"Ep: {epoch+1} (Step {global_step:06d})\"\n",
    "                    f\"Train loss {res['train_loss']:.3f}, val loss: {res['val_loss']:.3f},\"\n",
    "                    f\"Train reward margins {train_reward_margin:.3f} \"\n",
    "                    f\"Val reward margin: {val_reward_margin:.3f}\"\n",
    "                )\n",
    "\n",
    "                generate_model_output(\n",
    "                    model=policy_model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    data=start_context,\n",
    "                    device=loss.device,\n",
    "                )\n",
    "\n",
    "    return tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "res = evaluate_dpo_loss_loader(\n",
    "    policy_model=policy_model,\n",
    "    reference_model=reference_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    beta=0.1,\n",
    "    eval_iter=5\n",
    ")\n",
    "\n",
    "print('Training loss: ', res['train_loss'])\n",
    "print('val loss: ', res['val_loss'])\n",
    "(res['train_chosen_reward'] - res['train_rejected_reward']), (res['val_chosen_reward'] - res['val_rejected_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in val_data[5:7]:\n",
    "    input_text = format_input(data)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=policy_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG['context_length'],\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {data['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text}\")\n",
    "    print(\"\\n----------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "optimizer = torch.optim.AdamW(policy_model.parameters(), lr=5e-6, weight_decay=0.01)\n",
    "num_epochs = 1\n",
    "tracking = train_model_dpo_simple(\n",
    "    policy_model=policy_model,\n",
    "    reference_model=reference_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    beta=0.1,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=val_data[2],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_mins = (end_time - start_time)/60\n",
    "print(f\"{execution_time_mins:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tracking['train_losses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import plot_losses\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(tracking['train_losses']))\n",
    "plot_losses(\n",
    "    epochs_seen=epochs_tensor,\n",
    "    tokens_seen=tracking['tokens_seen'],\n",
    "    train_losses=tracking['train_losses'],\n",
    "    val_losses=tracking['val_losses'],\n",
    "    label='loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rewards_margins = [i-j for i, j in zip(tracking['train_chosen_rewards'], tracking['train_rejected_rewards'])]\n",
    "val_reward_margins = [i-j for i, j in zip(tracking['val_chosen_rewards'], tracking['val_rejected_rewards'])]\n",
    "\n",
    "plot_losses(\n",
    "    epochs_seen=epochs_tensor,\n",
    "    tokens_seen=tracking['tokens_seen'],\n",
    "    train_losses=train_rewards_margins,\n",
    "    val_losses=val_reward_margins,\n",
    "    label='loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in val_data[:10]:\n",
    "    input_text = format_input(data)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=reference_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG['context_length'],\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    ref_response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=policy_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG['context_length'],\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    policy_response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {data['output']}\")\n",
    "    print(f\"\\nReference Model response:\\n>> {ref_response_text}\")\n",
    "    print(f\"\\nPolicy Model response:\\n>> {policy_response_text}\")\n",
    "    print(\"\\n----------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_data[:5]:\n",
    "    input_text = format_input(data)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=reference_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG['context_length'],\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    ref_response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=policy_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG['context_length'],\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    policy_response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {data['output']}\")\n",
    "    print(f\"\\nReference Model response:\\n>> {ref_response_text}\")\n",
    "    print(f\"\\nPolicy Model response:\\n>> {policy_response_text}\")\n",
    "    print(\"\\n----------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
