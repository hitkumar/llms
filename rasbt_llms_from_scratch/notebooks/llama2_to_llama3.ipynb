{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from llama_modules import compute_rope, FeedForward, RMSNorm\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_scaling(\n",
        "    freqs: torch.tensor,\n",
        "    scale_factor: float,\n",
        "    low_freq_factor: float,\n",
        "    high_freq_factor: float,\n",
        "    old_context_len: int,\n",
        ") -> torch.tensor:\n",
        "    low_freq_wavelen = old_context_len / low_freq_factor\n",
        "    high_freq_wavelen = old_context_len / high_freq_factor\n",
        "    new_freqs = []\n",
        "    for i, freq in enumerate(freqs):\n",
        "        wavelen = 2 * math.pi / freq\n",
        "        if wavelen < high_freq_wavelen:\n",
        "            new_freqs.append(freq)\n",
        "        elif wavelen > low_freq_wavelen:\n",
        "            new_freqs.append(freq / scale_factor)\n",
        "        else:\n",
        "            assert low_freq_wavelen != high_freq_wavelen\n",
        "            smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n",
        "            new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)\n",
        "\n",
        "\n",
        "    return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "theta_base = 10_000\n",
        "head_dim = 32\n",
        "freqs = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
        "freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "apply_scaling(freqs, 8.0, 1.0, 4.0, 8192)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096, freq_config=None):\n",
        "    assert head_dim % 2 == 0, \"Embedding dimension should be even\"\n",
        "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
        "\n",
        "    if freq_config is not None:\n",
        "        inv_freq = apply_scaling(\n",
        "            inv_freq,\n",
        "            scale_factor=freq_config[\"factor\"],\n",
        "            low_freq_factor=freq_config[\"low_freq_factor\"],\n",
        "            high_freq_factor=freq_config[\"high_freq_factor\"],\n",
        "            old_context_len=freq_config[\"original_context_length\"]\n",
        "        )\n",
        "    positions = torch.arange(context_length)\n",
        "\n",
        "    angles = positions[:, None] * inv_freq[None, :]  # [context_length, head_dim/2]\n",
        "    angles = torch.cat([angles, angles], dim=1)  # [context_length, head_dim]\n",
        "    cos = torch.cos(angles)\n",
        "    sin = torch.sin(angles)\n",
        "\n",
        "    return cos, sin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Take from torchtune\n",
        "# def rope_scaling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llama_3_context_len = 8192 # 4192 for llama 2\n",
        "llama_3_theta_base = 500_000 # 10K for llama 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "num_heads = 4\n",
        "head_dim = 16\n",
        "\n",
        "cos, sin = precompute_rope_params(\n",
        "    head_dim=head_dim,\n",
        "    theta_base=llama_3_theta_base,\n",
        "    context_length=llama_3_context_len,\n",
        "    freq_config=None\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\n",
        "k = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\n",
        "q_rot = compute_rope(q, cos, sin)\n",
        "k_rot = compute_rope(k, cos, sin)\n",
        "q_rot.shape, k_rot.shape, q.shape, k.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SharedBuffers:\n",
        "    _buffers = {}\n",
        "\n",
        "    @staticmethod\n",
        "    def get_buffers(context_length, head_dim, rope_base, freq_config, dtype=torch.float32):\n",
        "        key = (context_length, head_dim, rope_base, tuple(freq_config.values()) if freq_config else freq_config, dtype)\n",
        "        if key not in SharedBuffers._buffers:\n",
        "            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "            cos, sin = precompute_rope_params(head_dim, rope_base, context_length, freq_config)\n",
        "            if dtype is not None:\n",
        "                cos = cos.to(dtype)\n",
        "                sin = sin.to(dtype)\n",
        "\n",
        "            SharedBuffers._buffers[key] = (mask, cos, sin)\n",
        "\n",
        "        return SharedBuffers._buffers[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "buf = SharedBuffers.get_buffers(12, 96, 10_000, None)\n",
        "buf[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, num_heads, num_kv_groups, rope_base=10_000, rope_config=None, dtype=None):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0\n",
        "        assert num_heads % num_kv_groups == 0\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
        "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
        "        self.num_kv_groups = num_kv_groups\n",
        "        self.group_size = num_heads // num_kv_groups\n",
        "\n",
        "        # not grouped\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "        self.out_proj = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "\n",
        "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, rope_base, rope_config, dtype)\n",
        "        self.register_buffer(\"mask\", mask)\n",
        "        self.register_buffer(\"cos\", cos)\n",
        "        self.register_buffer(\"sin\", sin)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        queries = self.W_query(x) # [b, num_tokens, d_out]\n",
        "        keys = self.W_key(x) # [b, num_tokens, num_kv_groups * head_dim]\n",
        "        values = self.W_value(x) # [b, num_tokens, num_kv_groups * head_dim]\n",
        "\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2) # [b, num_heads, num_tokens, head_dim]\n",
        "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2) # [b, num_kv_groups, num_tokens, head_dim]\n",
        "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2) # [b, num_kv_groups, num_tokens, head_dim]\n",
        "\n",
        "        # Apply ROPE\n",
        "        keys = compute_rope(keys, self.cos, self.sin)\n",
        "        queries = compute_rope(queries, self.cos, self.sin)\n",
        "\n",
        "        # [b, num_heads, num_tokens, head_dim]\n",
        "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
        "        values = values.repeat_interleave(self.group_size, dim=1)\n",
        "\n",
        "        # [b, num_heads, num_tokens, head_dim] [b, num_heads, head_dim, num_tokens] -> [b, num_heads, num_tokens, num_tokens]\n",
        "        attn_scores = torch.matmul(queries, keys.transpose(2, 3))\n",
        "        attn_scores = attn_scores / self.head_dim ** 0.5\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        # [b, num_heads, num_tokens, num_tokens]\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        # [b, num_heads, num_tokens, head_dim]\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
        "        return self.out_proj(context_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.arange(16).view(1, 4, 4)\n",
        "a.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b = a.repeat_interleave(2, dim=1)\n",
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_dim = 4096\n",
        "num_heads = 32\n",
        "max_context_length = 8192\n",
        "context_len = 3000\n",
        "batch_size = 2\n",
        "\n",
        "example_batch = torch.randn(batch_size, context_len, embed_dim)\n",
        "print(example_batch.shape)\n",
        "\n",
        "grouped_query_attention = GroupedQueryAttention(\n",
        "    d_in=embed_dim,\n",
        "    d_out=embed_dim,\n",
        "    context_length=max_context_length,\n",
        "    num_heads=num_heads,\n",
        "    num_kv_groups=8,\n",
        "    rope_base=llama_3_theta_base\n",
        ")\n",
        "\n",
        "print(grouped_query_attention(example_batch).shape)\n",
        "print(grouped_query_attention.W_key.weight.shape)\n",
        "print(grouped_query_attention.W_query.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del grouped_query_attention\n",
        "del example_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    Apply RMSNorm and residual connection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.norm = RMSNorm(size)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + sublayer(self.norm(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = GroupedQueryAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
        "            rope_base=cfg['rope_base'],\n",
        "            rope_config=cfg['rope_freq'],\n",
        "            dtype=cfg['dtype']\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.sublayer1 = SublayerConnection(cfg[\"emb_dim\"])\n",
        "        self.sublayer2 = SublayerConnection(cfg[\"emb_dim\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # might have some interesting consequences when we load weights\n",
        "        # attention block\n",
        "        x = self.sublayer1(x, self.att)\n",
        "        # FF block\n",
        "        x = self.sublayer2(x, self.ff)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Llama3Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(\n",
        "            cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"]\n",
        "        )\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"]\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx, targets=None):\n",
        "        x = self.tok_emb(in_idx)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLAMA3_CONFIG_8B = {\n",
        "    \"vocab_size\": 128_256,\n",
        "    \"context_length\": 8192,\n",
        "    \"emb_dim\": 4096,\n",
        "    \"n_heads\": 32,\n",
        "    \"n_layers\": 32,\n",
        "    \"hidden_dim\": 14_336,\n",
        "    \"n_kv_groups\": 8,\n",
        "    \"rope_base\": 500_000,\n",
        "    \"rope_freq\": None,\n",
        "    \"dtype\": torch.bfloat16\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Llama3Model(LLAMA3_CONFIG_8B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 825525389611937,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "print(model.trf_blocks[0].att.mask is model.trf_blocks[-1].att.mask)\n",
        "print(model.trf_blocks[0].att.cos is model.trf_blocks[-1].att.cos)\n",
        "print(model.trf_blocks[0].att.sin is model.trf_blocks[-1].att.sin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model_utils import total_memory_size, get_model_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1099265608586463,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "total_memory_size(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 590638527249837,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "total_memory_size(model, torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 535322392820976,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "get_model_params(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import tiktoken\n",
        "from tiktoken.load import load_tiktoken_bpe\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, model_path):\n",
        "        assert os.path.isfile(model_path), f\"Model file {model_path} not found\"\n",
        "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
        "\n",
        "        self.special_tokens = {\n",
        "            \"<|begin_of_text|>\": 128000,\n",
        "            \"<|end_of_text|>\": 128001,\n",
        "            \"<|start_header_id|>\": 128006,\n",
        "            \"<|end_header_id|>\": 128007,\n",
        "            \"<|eot_id|>\": 128009,\n",
        "        }\n",
        "        self.special_tokens.update({\n",
        "            f\"<|reserved_{i}|>\": 128002 + i for i in range(256) if (128002 + i) not in self.special_tokens.values()\n",
        "        })\n",
        "\n",
        "        self.model = tiktoken.Encoding(\n",
        "            name=Path(model_path).name,\n",
        "            pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
        "            mergeable_ranks=mergeable_ranks,\n",
        "            special_tokens=self.special_tokens\n",
        "        )\n",
        "\n",
        "    def encode(self, text, bos=False, eos=False, allowed_special=set(), disallowed_special=()):\n",
        "        tokens = []\n",
        "        if bos:\n",
        "            tokens.append(self.special_tokens[\"<|begin_of_text|>\"])\n",
        "\n",
        "        tokens += self.model.encode(text, allowed_special=allowed_special, disallowed_special=disallowed_special)\n",
        "\n",
        "        if eos:\n",
        "            tokens.append(self.special_tokens[\"<|end_of_text|>\"])\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return self.model.decode(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(\"/home/htkumar/llms/Llama-3-8B/original/tokenizer.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1285576192784207,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(tokenizer.encode(\"hello world\", bos=True, eos=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1105046091072645,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_model import generate, text_to_token_ids, token_ids_to_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 2077890179337244,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
        "    max_new_tokens=30,\n",
        "    context_size=LLAMA3_CONFIG_8B[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1123075855866581,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "model_path = \"/home/htkumar/llms/Llama-3-8B\"\n",
        "os.path.join(model_path, \"original\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from safetensors.torch import load_file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "combined_weights = {}\n",
        "\n",
        "for i in range(1, 5):\n",
        "    weights_file = os.path.join(model_path, f\"model-0000{i}-of-00004.safetensors\")\n",
        "    current_weights = load_file(weights_file)\n",
        "    combined_weights.update(current_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 593950589713787,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "list(combined_weights.keys())[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 891139603235035,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "combined_weights['model.embed_tokens.weight'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1475890293106281,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "combined_weights['lm_head.weight'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_model import assign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_weights_into_llama3(model, param_config, params):\n",
        "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"])\n",
        "\n",
        "    for l in range(param_config['n_layers']):\n",
        "        # Load att weights\n",
        "        model.trf_blocks[l].att.W_query.weight = assign(\n",
        "            model.trf_blocks[l].att.W_query.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
        "        )\n",
        "        model.trf_blocks[l].att.W_key.weight = assign(\n",
        "            model.trf_blocks[l].att.W_key.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
        "        )\n",
        "        model.trf_blocks[l].att.W_value.weight = assign(\n",
        "            model.trf_blocks[l].att.W_value.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
        "        )\n",
        "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
        "            model.trf_blocks[l].att.out_proj.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
        "        )\n",
        "        model.trf_blocks[l].sublayer1.norm.weight = assign(\n",
        "            model.trf_blocks[l].sublayer1.norm.weight,\n",
        "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
        "        )\n",
        "\n",
        "        # Load FF weights\n",
        "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc1.weight,\n",
        "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
        "        )\n",
        "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc2.weight,\n",
        "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
        "        )\n",
        "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc3.weight,\n",
        "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
        "        )\n",
        "        model.trf_blocks[l].sublayer2.norm.weight = assign(\n",
        "            model.trf_blocks[l].sublayer2.norm.weight,\n",
        "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
        "        )\n",
        "\n",
        "    # Load output layer weights\n",
        "    model.final_norm.weight = assign(model.final_norm.weight, params['model.norm.weight'])\n",
        "\n",
        "    if 'lm_head.weight' in params.keys():\n",
        "        model.out_head.weight = assign(model.out_head.weight, params['lm_head.weight'])\n",
        "    else:\n",
        "        # weight tying\n",
        "        model.out_head.weight = assign(model.out_head.weight, params['model.embed_tokens.weight'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_weights_into_llama3(model, LLAMA3_CONFIG_8B, combined_weights)\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del combined_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1107301120744914,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
        "    max_new_tokens=30,\n",
        "    context_size=LLAMA3_CONFIG_8B[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# free memory\n",
        "import gc\n",
        "\n",
        "del model\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = \"/home/htkumar/llms/Llama-3-8B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_weights = {}\n",
        "for i in range(1, 5):\n",
        "    weights_file = os.path.join(model_path, f\"model-0000{i}-of-00004.safetensors\")\n",
        "    current_weights = load_file(weights_file)\n",
        "    combined_weights.update(current_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 2335676550106687,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "list(combined_weights.keys())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Llama3Model(LLAMA3_CONFIG_8B)\n",
        "load_weights_into_llama3(model, LLAMA3_CONFIG_8B, combined_weights)\n",
        "model.to(device);\n",
        "del combined_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChatFormat:\n",
        "    def __init__(self, tokenizer):\n",
        "        self._tokenizer = tokenizer\n",
        "\n",
        "    def encode_header(self, message):\n",
        "        tokens = []\n",
        "        tokens.append(self._tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
        "        tokens.extend(self._tokenizer.encode(message[\"role\"], bos=False, eos=False))\n",
        "        tokens.append(self._tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
        "        tokens.extend(self._tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, text):\n",
        "        message = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": text\n",
        "        }\n",
        "\n",
        "        tokens = self.encode_header(message)\n",
        "        tokens.extend(\n",
        "            self._tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n",
        "        )\n",
        "        tokens.append(self._tokenizer.special_tokens[\"<|eot_id|>\"])\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        return self._tokenizer.decode(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_tokenizer = ChatFormat(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1124929685651817,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "token_ids = chat_tokenizer.encode(\"Hello world!\")\n",
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 4027158627567753,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "tokenizer.special_tokens[\"<|eot_id|>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1012018850755249,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "chat_tokenizer.decode(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 963124982618209,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "chat_tokenizer.decode(chat_tokenizer.encode(\"What do llamas eat?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1310820180274086,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "token_ids_to_text((text_to_token_ids(\"What do llamas eat?\", chat_tokenizer)), chat_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1955070684904171,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"What do llamas eat?\", chat_tokenizer).to(device),\n",
        "    max_new_tokens=150,\n",
        "    context_size=LLAMA3_CONFIG_8B[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, chat_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_text = token_ids_to_text(token_ids, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text, header_end=\"assistant<|end_header_id|>\\n\\n\"):\n",
        "    index = text.find(header_end)\n",
        "    if index != -1:\n",
        "        return text[index + len(header_end):].strip()\n",
        "    else:\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1369425647377955,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "print(clean_text(output_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model_utils import free_pytorch_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "free_pytorch_memory(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLAMA 3.1 and above models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLAMA31_CONFIG_8B = {\n",
        "    \"vocab_size\": 128_256,\n",
        "    \"context_length\": 131_072,  # increased\n",
        "    \"emb_dim\": 4096,\n",
        "    \"n_heads\": 32,\n",
        "    \"n_layers\": 32,\n",
        "    \"hidden_dim\": 14_336,\n",
        "    \"n_kv_groups\": 8,\n",
        "    \"rope_base\": 500_000,\n",
        "    \"dtype\": torch.bfloat16,\n",
        "    \"rope_freq\": {\n",
        "        \"factor\": 8.0,\n",
        "        \"low_freq_factor\": 1.0,\n",
        "        \"high_freq_factor\": 4.0,\n",
        "        \"original_context_length\": 8192,\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "old_context_length = LLAMA31_CONFIG_8B[\"context_length\"]\n",
        "LLAMA31_CONFIG_8B[\"context_length\"] = 8192\n",
        "\n",
        "\n",
        "def rescale_theta(theta_old, context_length_old, context_length_new):\n",
        "    scaling_factor = context_length_new / context_length_old\n",
        "    theta_new = theta_old * scaling_factor\n",
        "    return theta_new\n",
        "\n",
        "LLAMA31_CONFIG_8B[\"rope_base\"] = rescale_theta(\n",
        "    LLAMA31_CONFIG_8B[\"rope_base\"],\n",
        "    old_context_length,\n",
        "    LLAMA31_CONFIG_8B[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"New RoPE theta:\", LLAMA31_CONFIG_8B[\"rope_base\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llama_31_8b_model_path = \"/home/htkumar/llms/Llama-3.1-8B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(os.path.join(llama_31_8b_model_path, \"original\", \"tokenizer.model\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.encode(\"How are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Llama3Model(LLAMA31_CONFIG_8B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model_utils import get_model_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(get_model_params(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_weights = {}\n",
        "for i in range(1, 5):\n",
        "    weights_file = os.path.join(llama_31_8b_model_path, f\"model-0000{i}-of-00004.safetensors\")\n",
        "    current_weights = load_file(weights_file)\n",
        "    combined_weights.update(current_weights)\n",
        "\n",
        "load_weights_into_llama3(model, LLAMA31_CONFIG_8B, combined_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.to(device);\n",
        "del combined_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
        "    max_new_tokens=30,\n",
        "    context_size=LLAMA31_CONFIG_8B[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model_utils import free_pytorch_memory\n",
        "free_pytorch_memory(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLAMA32_CONFIG_1B = {\n",
        "    \"vocab_size\": 128_256,\n",
        "    \"context_length\": 131_072,\n",
        "    \"emb_dim\": 2048,\n",
        "    \"n_heads\": 32,\n",
        "    \"n_layers\": 16,\n",
        "    \"hidden_dim\": 8192,\n",
        "    \"n_kv_groups\": 8,\n",
        "    \"rope_base\": 500_000,\n",
        "    \"dtype\": torch.bfloat16,\n",
        "    \"rope_freq\": {\n",
        "        \"factor\": 32.0,\n",
        "        \"low_freq_factor\": 1.0,\n",
        "        \"high_freq_factor\": 4.0,\n",
        "        \"original_context_length\": 8192,\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "old_context_length = LLAMA32_CONFIG_1B[\"context_length\"]\n",
        "LLAMA32_CONFIG_1B[\"context_length\"] = 8192\n",
        "\n",
        "LLAMA32_CONFIG_1B[\"rope_base\"] = rescale_theta(\n",
        "    LLAMA32_CONFIG_1B[\"rope_base\"],\n",
        "    old_context_length,\n",
        "    LLAMA32_CONFIG_1B[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"New RoPE theta:\", LLAMA32_CONFIG_1B[\"rope_base\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llama32_1b_model_path = \"/home/htkumar/llms/Llama-3.2-1B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(os.path.join(llama32_1b_model_path, \"original\", \"tokenizer.model\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.encode(\"How are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights_file = os.path.join(llama32_1b_model_path, \"model.safetensors\")\n",
        "current_weights = load_file(weights_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Llama3Model(LLAMA32_CONFIG_1B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_model_params(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_weights_into_llama3(model, LLAMA32_CONFIG_1B, current_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del current_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.equal(model.tok_emb.weight, model.out_head.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
        "    max_new_tokens=30,\n",
        "    context_size=LLAMA32_CONFIG_1B[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "free_pytorch_memory(model);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "af4c12b5-2f6c-4c9a-92e2-f89b6d2e7d7a",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "gpt-dev (local)",
      "language": "python",
      "name": "gpt-dev_local"
    },
    "language_info": {
      "name": "plaintext"
    }
  }
}
