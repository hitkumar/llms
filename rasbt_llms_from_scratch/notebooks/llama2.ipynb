{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 909018124524967,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "from importlib.metadata import version\n",
        "version(\"sentencepiece\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Check the dtype calculation here.\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, emb_dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.emb_dim = emb_dim\n",
        "        self.weight = nn.Parameter(torch.ones(emb_dim)).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        rms_mean = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "        x_norm = x * torch.rsqrt(rms_mean + self.eps)\n",
        "        return (x_norm * self.weight).to(dtype=x.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1739661910206353,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(\n",
        "    [\n",
        "        [1, 2, 3],\n",
        "        [4, 5, 6]\n",
        "    ]\n",
        ").float()\n",
        "rms_norm = RMSNorm(emb_dim=3)\n",
        "rms_norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 462797589635152,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "rms_norm_pytorch = nn.RMSNorm(x.shape[-1], eps=1e-5)\n",
        "rms_norm_pytorch(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SiLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1608874379981916,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "silu = SiLU()\n",
        "silu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1071981214713357,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "F.silu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg['emb_dim'], cfg['hidden_dim'], dtype=cfg['dtype'], bias=False)\n",
        "        self.fc2 = nn.Linear(cfg['emb_dim'], cfg['hidden_dim'], dtype=cfg['dtype'], bias=False)\n",
        "        self.fc3 = nn.Linear(cfg['hidden_dim'], cfg['emb_dim'], dtype=cfg['dtype'], bias=False)\n",
        "        self.silu = SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_fc1 = self.fc1(x)\n",
        "        x_fc2 = self.fc2(x)\n",
        "        return self.fc3(self.silu(x_fc1) * x_fc2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLAMA2_CONFIG_7B = {\n",
        "    \"vocab_size\": 32000,     # Vocabulary size\n",
        "    \"context_length\": 4096,  # Context length\n",
        "    \"emb_dim\": 4096,         # Embedding dimension\n",
        "    \"n_heads\": 32,           # Number of attention heads\n",
        "    \"n_layers\": 32,          # Number of layers\n",
        "    \"hidden_dim\": 11008,     # NEW: Size of the intermediate dimension in FeedForward\n",
        "    \"dtype\": torch.bfloat16  # NEW: Lower-precision dtype to reduce memory usage\n",
        "}\n",
        "feed_forward = FeedForward(LLAMA2_CONFIG_7B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn((4, 8, 4096), dtype=torch.bfloat16)\n",
        "feed_forward(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096):\n",
        "    assert head_dim %2 == 0, \"Embedding dimension should be even\"\n",
        "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
        "    positions = torch.arange(context_length)\n",
        "\n",
        "    angles = positions[:, None] * inv_freq[None, :] # [context_length, head_dim/2]\n",
        "    angles = torch.cat([angles, angles], dim=1) # [context_length, head_dim]\n",
        "    cos = torch.cos(angles)\n",
        "    sin = torch.sin(angles)\n",
        "\n",
        "    return cos, sin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_rope(x, cos, sin):\n",
        "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
        "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "    assert head_dim %2 == 0, \"Embedding dimension should be even\"\n",
        "    x1 = x[..., :head_dim//2]\n",
        "    x2 = x[..., head_dim//2:]\n",
        "    rotated = torch.cat((-x2, x1), dim=-1)\n",
        "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    x_rotated = (x * cos) + (rotated * sin)\n",
        "    return x_rotated.to(dtype=x.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_dim = 2\n",
        "context_len = 5\n",
        "num_heads = 4\n",
        "head_dim = 16\n",
        "\n",
        "cos, sin = precompute_rope_params(16, context_length=5)\n",
        "q = torch.ones((batch_dim, num_heads, context_len, head_dim))\n",
        "q_rotated = compute_rope(q, cos, sin)\n",
        "q_rotated.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[0, 0, 2, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_rotated[0, 0, 2, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, num_heads, dtype=None):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
        "        self.register_buffer(\n",
        "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "        cos, sin = precompute_rope_params(head_dim=self.head_dim, context_length=context_length)\n",
        "        self.register_buffer(\"cos\", cos)\n",
        "        self.register_buffer(\"sin\", sin)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # transpose (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Apply rope here.\n",
        "        keys = compute_rope(keys, self.cos, self.sin)\n",
        "        queries = compute_rope(queries, self.cos, self.sin)\n",
        "\n",
        "        # dot product for each head\n",
        "        attn_scores = torch.matmul(\n",
        "            queries, keys.transpose(2, 3)\n",
        "        )  # (b, num_heads, num_tokens, num_tokens) # double check this\n",
        "        mask = self.mask[:num_tokens, :num_tokens].bool()\n",
        "        attn_scores.masked_fill_(mask, -torch.inf)\n",
        "\n",
        "        scaled_attn_scores = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
        "\n",
        "        # (b, num_heads, num_tokens, num_tokens) *\n",
        "        # (b, num_heads, num_tokens, head_dim) -> (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (scaled_attn_scores @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "        return context_vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mha = MultiHeadAttention(\n",
        "    d_in=128,\n",
        "    d_out=128,\n",
        "    context_length=100,\n",
        "    num_heads=4\n",
        ")\n",
        "\n",
        "example_batch = torch.randn((1, 100, 128))\n",
        "print(example_batch.shape)\n",
        "mha(example_batch).shape\n",
        "\n",
        "del example_batch, mha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    Apply RMSNorm and residual connection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.norm = RMSNorm(size)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + sublayer(self.norm(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dtype=cfg[\"dtype\"],\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.sublayer1 = SublayerConnection(cfg[\"emb_dim\"])\n",
        "        self.sublayer2 = SublayerConnection(cfg[\"emb_dim\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # might have some interesting consequences when we load weights\n",
        "        # attention block\n",
        "        x = self.sublayer1(x, self.att)\n",
        "        # FF block\n",
        "        x = self.sublayer2(x, self.ff)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Llama2Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        _, seq_len = in_idx.shape\n",
        "        x = self.tok_emb(in_idx)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLAMA2_CONFIG_7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Llama2Model(LLAMA2_CONFIG_7B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.tensor(0, dtype=torch.float32).element_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.tensor(0, dtype=torch.bfloat16).element_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def total_memory_size(model, input_dtype=torch.float32):\n",
        "    total_params = 0\n",
        "    total_grads = 0\n",
        "    for param in model.parameters():\n",
        "        param_size = param.numel()\n",
        "        total_params += param_size\n",
        "        if param.requires_grad:\n",
        "            total_grads += param_size\n",
        "\n",
        "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
        "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
        "\n",
        "    model_size_bytes = (total_params + total_grads + total_buffers) * element_size\n",
        "    model_size_gb = model_size_bytes / (2 ** 30)\n",
        "    return model_size_gb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_memory_size(model, input_dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_memory_size(model, input_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download the tokenizer file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "class LLamaTokenizer:\n",
        "    def __init__(self, tokenizer_file):\n",
        "        sp = spm.SentencePieceProcessor()\n",
        "        sp.load(tokenizer_file)\n",
        "        self.tokenizer = sp\n",
        "\n",
        "    def encode(self, text):\n",
        "        return self.tokenizer.encode_as_ids(text)\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return self.tokenizer.decode_pieces(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer_file = \"/home/htkumar/llms/llama-2-7b/tokenizer.model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = LLamaTokenizer(tokenizer_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_model import generate, text_to_token_ids, token_ids_to_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids1(\"Every effort moves\", tokenizer).to(device),\n",
        "    max_new_tokens=30,\n",
        "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "token_ids_to_text(token_ids, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights_file = \"/home/htkumar/llms/llama-2-7b/consolidated.00.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights = torch.load(weights_file, weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(weights.keys())[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch, left: {left.shape}, right: {right.shape}\")\n",
        "\n",
        "    if isinstance(right, torch.Tensor):\n",
        "        return torch.nn.Parameter(right.clone().detach())\n",
        "    else:\n",
        "        return torch.nn.Parameter(torch.tensor(right))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_weights_into_llama(model, param_config, params):\n",
        "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"tok_embeddings.weight\"])\n",
        "\n",
        "    for l in range(param_config['n_layers']):\n",
        "        # Load att weights\n",
        "        model.trf_blocks[l].att.W_query.weight = assign(\n",
        "            model.trf_blocks[l].att.W_query.weight,\n",
        "            params[f'layers.{l}.attention.wq.weight']\n",
        "        )\n",
        "        model.trf_blocks[l].att.W_key.weight = assign(\n",
        "            model.trf_blocks[l].att.W_key.weight,\n",
        "            params[f'layers.{l}.attention.wk.weight']\n",
        "        )\n",
        "        model.trf_blocks[l].att.W_value.weight = assign(\n",
        "            model.trf_blocks[l].att.W_value.weight,\n",
        "            params[f'layers.{l}.attention.wv.weight']\n",
        "        )\n",
        "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
        "            model.trf_blocks[l].att.out_proj.weight,\n",
        "            params[f'layers.{l}.attention.wo.weight']\n",
        "        )\n",
        "        model.trf_blocks[l].sublayer1.norm.weight = assign(\n",
        "            model.trf_blocks[l].sublayer1.norm.weight,\n",
        "            params[f'layers.{l}.attention_norm.weight']\n",
        "        )\n",
        "\n",
        "        # Load FF weights\n",
        "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc1.weight,\n",
        "            params[f'layers.{l}.feed_forward.w1.weight']\n",
        "        )\n",
        "        # For some reason w2 and w3 are provided in the wrong order in the weights file\n",
        "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc2.weight,\n",
        "            params[f\"layers.{l}.feed_forward.w3.weight\"]\n",
        "        )\n",
        "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc3.weight,\n",
        "            params[f\"layers.{l}.feed_forward.w2.weight\"]\n",
        "        )\n",
        "        model.trf_blocks[l].sublayer2.norm.weight = assign(\n",
        "            model.trf_blocks[l].sublayer2.norm.weight,\n",
        "            params[f'layers.{l}.ffn_norm.weight']\n",
        "        )\n",
        "\n",
        "    # Load output layer weights\n",
        "    model.final_norm.weight = assign(model.final_norm.weight, params['norm.weight'])\n",
        "    model.out_head.weight = assign(model.out_head.weight, params['output.weight'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_weights_into_llama(model, LLAMA2_CONFIG_7B, weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids1(\"Every effort\", tokenizer).to(device),\n",
        "    max_new_tokens=30,\n",
        "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights_file_chat = \"/home/htkumar/llms/llama-2-7b-chat/consolidated.00.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights_chat = torch.load(weights_file_chat, weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_chat = Llama2Model(LLAMA2_CONFIG_7B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_weights_into_llama(model_chat, LLAMA2_CONFIG_7B, weights_chat)\n",
        "model_chat.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "    model=model_chat,\n",
        "    idx=text_to_token_ids1(\"What do llamas eat?\", tokenizer).to(device),\n",
        "    max_new_tokens=30,\n",
        "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model_chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# last cell, clean up memory\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "67770a70-4af4-41a5-b76a-7ab6bdafd5a9",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "gpt-dev (local)",
      "language": "python",
      "name": "gpt-dev_local"
    },
    "language_info": {
      "name": "plaintext"
    }
  }
}
