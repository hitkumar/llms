{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tensor0d = torch.tensor(1)\n",
    "tensor0d.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor0d.dtype # default data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floatvec = tensor0d.to(torch.float32)\n",
    "floatvec.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_nn = Linear(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in linear_nn.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_nn.linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5425,  0.2925, -0.7646,  0.3795, -0.5639, -0.7500, -0.6345,  0.0665,\n",
       "          0.4084,  0.6569,  0.4079,  0.0801,  0.1203,  0.7781,  0.5231,  0.6234,\n",
       "          0.2967,  0.5512,  1.0278,  0.3727],\n",
       "        [ 0.1335, -0.2083,  0.0917,  0.5566, -0.2087, -0.0329, -0.8339, -0.6418,\n",
       "          0.1521,  0.3822, -0.0745, -0.2706,  0.2469, -0.0625, -0.6467, -0.2150,\n",
       "         -0.1824, -0.2962,  0.1416,  0.1111],\n",
       "        [-0.3351,  0.1941,  0.7869, -0.0146, -0.1057, -0.5422,  0.5865,  0.1522,\n",
       "         -1.0977, -0.6273,  0.8772,  0.1482, -0.2450,  0.3409,  1.1632,  0.2264,\n",
       "          0.1852,  0.5383,  0.4046,  0.2762],\n",
       "        [ 1.0684,  0.2418, -1.1733,  1.4849,  0.2549, -0.5645, -1.9621,  0.6086,\n",
       "         -0.1669,  1.5709,  0.3428, -0.3172,  1.1161, -0.4844,  0.6596, -0.3492,\n",
       "          0.2490,  1.4173,  0.7968,  0.8737]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(4, 10)\n",
    "linear_nn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_higest_gradient(model):\n",
    "    max_grad = None\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_values = param.grad.data.flatten()\n",
    "            max_grad_param = torch.max(grad_values)\n",
    "            if max_grad is None or max_grad_param > max_grad:\n",
    "                max_grad = max_grad_param\n",
    "    return max_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_higest_gradient(linear_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
