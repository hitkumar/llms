{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "matplotlib version: 3.7.2\n",
            "numpy version: 1.24.3\n",
            "tiktoken version: 0.6.0\n",
            "torch version: 2.0.1\n",
            "tensorflow version: 2.13.0\n",
            "pandas version: 2.0.3\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\"matplotlib\", \"numpy\", \"tiktoken\", \"torch\", \"tensorflow\", \"pandas\"]\n",
        "\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('sms_spam_collection/SMSSpamCollection.tsv')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "data_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_and_unzip(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists, no need to download\")\n",
        "        return\n",
        "\n",
        "    # download the file\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    # unzip the file\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    # add .tsv extension\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sms_spam_collection/SMSSpamCollection.tsv already exists, no need to download\n"
          ]
        }
      ],
      "source": [
        "download_and_unzip(url, zip_path, extracted_path, data_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"label\", \"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5572, 2)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "ham     4825\n",
              "spam     747\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"label\"].value_counts()\n",
        "# unbalanced dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(747, 2)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[df.label == \"spam\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_balanced_dataset(df):\n",
        "    num_spam = df[df.label == \"spam\"].shape[0]\n",
        "    ham_subset = df[df.label == \"ham\"].sample(num_spam, random_state=123)\n",
        "    balanced_df = pd.concat([ham_subset, df[df.label == \"spam\"]])\n",
        "    return balanced_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "ham     747\n",
              "spam    747\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "balanced_df = create_balanced_dataset(df)\n",
        "balanced_df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1494, 2)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "balanced_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "balanced_df[\"label\"] = balanced_df[\"label\"].map({\"ham\": 0, \"spam\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>WINNER!! As a valued network customer you have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5537</th>\n",
              "      <td>1</td>\n",
              "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5540</th>\n",
              "      <td>1</td>\n",
              "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5547</th>\n",
              "      <td>1</td>\n",
              "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5566</th>\n",
              "      <td>1</td>\n",
              "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>1</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>747 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text\n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "5         1  FreeMsg Hey there darling it's been 3 week's n...\n",
              "8         1  WINNER!! As a valued network customer you have...\n",
              "9         1  Had your mobile 11 months or more? U R entitle...\n",
              "11        1  SIX chances to win CASH! From 100 to 20,000 po...\n",
              "...     ...                                                ...\n",
              "5537      1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
              "5540      1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
              "5547      1  Had your contract mobile 11 Mnths? Latest Moto...\n",
              "5566      1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
              "5567      1  This is the 2nd time we have tried 2 contact u...\n",
              "\n",
              "[747 rows x 2 columns]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "balanced_df[balanced_df.label == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1340</td>\n",
              "      <td>0</td>\n",
              "      <td>Every monday..nxt week vl be completing..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5164</td>\n",
              "      <td>1</td>\n",
              "      <td>Congrats 2 mobile 3G Videophones R yours. call...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>708</td>\n",
              "      <td>0</td>\n",
              "      <td>Quite late lar... Ard 12 anyway i wun b drivin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3266</td>\n",
              "      <td>1</td>\n",
              "      <td>44 7732584351, Do you want a New Nokia 3510i c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>929</td>\n",
              "      <td>0</td>\n",
              "      <td>Oh all have to come ah?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1489</th>\n",
              "      <td>754</td>\n",
              "      <td>0</td>\n",
              "      <td>When did you get to the library</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1490</th>\n",
              "      <td>3174</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear Voucher Holder, To claim this weeks offer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1491</th>\n",
              "      <td>3921</td>\n",
              "      <td>1</td>\n",
              "      <td>FREE&gt;Ringtone! Reply REAL or POLY eg REAL1 1. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1492</th>\n",
              "      <td>3763</td>\n",
              "      <td>1</td>\n",
              "      <td>FREE for 1st week! No1 Nokia tone 4 ur mob eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1493</th>\n",
              "      <td>2119</td>\n",
              "      <td>1</td>\n",
              "      <td>U can WIN £100 of Music Gift Vouchers every we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1494 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  label                                               text\n",
              "0      1340      0          Every monday..nxt week vl be completing..\n",
              "1      5164      1  Congrats 2 mobile 3G Videophones R yours. call...\n",
              "2       708      0  Quite late lar... Ard 12 anyway i wun b drivin...\n",
              "3      3266      1  44 7732584351, Do you want a New Nokia 3510i c...\n",
              "4       929      0                            Oh all have to come ah?\n",
              "...     ...    ...                                                ...\n",
              "1489    754      0                    When did you get to the library\n",
              "1490   3174      1  Dear Voucher Holder, To claim this weeks offer...\n",
              "1491   3921      1  FREE>Ringtone! Reply REAL or POLY eg REAL1 1. ...\n",
              "1492   3763      1  FREE for 1st week! No1 Nokia tone 4 ur mob eve...\n",
              "1493   2119      1  U can WIN £100 of Music Gift Vouchers every we...\n",
              "\n",
              "[1494 rows x 3 columns]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "balanced_df.sample(frac=1).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['label', 'text'], dtype='object')"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "balanced_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "    # shuffle dataframe\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "    train_df = df[:train_end]\n",
        "    valid_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, valid_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1045, 2), (149, 2), (300, 2))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "train_df.shape, validation_df.shape, test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[50256]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2437, 389, 345, 1804]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(\"How are you doing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer.encode??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, csv_file, tokenizer, max_length=None, pad_token_id=50256\n",
        "    ) -> None:\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # pre-tokenizer texts\n",
        "        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"text\"]]\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "            # truncate texts if they are longer than max length\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[: self.max_length] for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "\n",
        "        # Add padding\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded_text = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"label\"]\n",
        "        return (\n",
        "            torch.tensor(encoded_text, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            if len(encoded_text) > max_length:\n",
        "                max_length = len(encoded_text)\n",
        "        return max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Alice'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = {\n",
        "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
        "    \"Age\": [25, 30, 35, 40],\n",
        "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.iloc[0][\"Name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "120\n"
          ]
        }
      ],
      "source": [
        "train_dataset = SpamDataset(csv_file=\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
        "print(train_dataset.max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer\n",
        ")\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(120, 120)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_dataset.max_length, test_dataset.max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=validation_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataLoader??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 120]) torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "for text, label in test_loader:\n",
        "    print(text.shape, label.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(130, 19, 38)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_loader), len(val_loader), len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('https://openaipublic.blob.core.windows.net/gpt-2/models/124M/checkpoint',\n",
              " 'downloaded_models/checkpoint')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "filename = \"checkpoint\"\n",
        "model_dir = \"downloaded_models\"\n",
        "model_size = \"124M\"\n",
        "\n",
        "file_url = os.path.join(base_url, model_size, filename)\n",
        "file_path = os.path.join(model_dir, filename)\n",
        "file_url, file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(model_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Content-Length': '77', 'Content-Type': 'application/octet-stream', 'Content-MD5': 'ygNo/NPEwamaykJRHQwfEg==', 'Last-Modified': 'Wed, 02 Dec 2020 17:33:04 GMT', 'ETag': '0x8D896E85318642F', 'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0', 'x-ms-request-id': '86baa400-101e-000d-3151-c2c15b000000', 'x-ms-version': '2009-09-19', 'x-ms-meta-Mtime': '2019-09-17T04:52:11.325000000Z', 'x-ms-lease-status': 'unlocked', 'x-ms-blob-type': 'BlockBlob', 'Date': 'Wed, 19 Jun 2024 14:00:14 GMT'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = requests.get(file_url, stream=True)\n",
        "response.headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sms+spam+collection.zip: 100%|██████████| 77.0/77.0 [00:00<00:00, 55.9kiB/s]\n"
          ]
        }
      ],
      "source": [
        "file_size = int(response.headers.get(\"content-length\", 0))\n",
        "block_size = 1024\n",
        "progress_bar_description = url.split(\"/\")[-1]\n",
        "with tqdm(\n",
        "    total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description\n",
        ") as progress_bar:\n",
        "    with open(file_path, \"wb\") as file:\n",
        "        for chunk in response.iter_content(block_size):\n",
        "            progress_bar.update(len(chunk))\n",
        "            file.write(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR:tensorflow:Couldn't match files for checkpoint downloaded_models/model.ckpt\n"
          ]
        }
      ],
      "source": [
        "tk_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "tk_ckpt_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_file(file_path, destination):\n",
        "    response = requests.get(file_path, stream=True)\n",
        "    file_size = int(response.headers.get(\"content-length\", 0))\n",
        "    if os.path.exists(destination):\n",
        "        file_size_local = os.path.getsize(destination)\n",
        "        if file_size == file_size_local:\n",
        "            print(f\"File exists: {destination}\")\n",
        "            return\n",
        "\n",
        "    block_size = 1024\n",
        "    progress_bar_description = file_path.split(\"/\")[-1]\n",
        "    with tqdm(\n",
        "        total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description\n",
        "    ) as progress_bar:\n",
        "        with open(destination, \"wb\") as file:\n",
        "            for chunk in response.iter_content(block_size):\n",
        "                progress_bar.update(len(chunk))\n",
        "                file.write(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    # model sizes available in https://github.com/openai/gpt-2/blob/master/DEVELOPERS.md\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size {model_size} not in {allowed_sizes}\")\n",
        "\n",
        "    # define paths\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    filenames = [\n",
        "        \"checkpoint\",\n",
        "        \"encoder.json\",\n",
        "        \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\",\n",
        "        \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\",\n",
        "        \"vocab.bpe\",\n",
        "    ]\n",
        "\n",
        "    # download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        print(file_url, file_path)\n",
        "        download_file(file_url, file_path)\n",
        "\n",
        "    # Add tf loading part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://openaipublic.blob.core.windows.net/gpt-2/models/124M/checkpoint gpt2/124M/checkpoint\n",
            "File exists: gpt2/124M/checkpoint\n",
            "https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json gpt2/124M/encoder.json\n",
            "File exists: gpt2/124M/encoder.json\n",
            "https://openaipublic.blob.core.windows.net/gpt-2/models/124M/hparams.json gpt2/124M/hparams.json\n",
            "File exists: gpt2/124M/hparams.json\n",
            "https://openaipublic.blob.core.windows.net/gpt-2/models/124M/model.ckpt.data-00000-of-00001 gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File exists: gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "https://openaipublic.blob.core.windows.net/gpt-2/models/124M/model.ckpt.index gpt2/124M/model.ckpt.index\n",
            "File exists: gpt2/124M/model.ckpt.index\n",
            "https://openaipublic.blob.core.windows.net/gpt-2/models/124M/model.ckpt.meta gpt2/124M/model.ckpt.meta\n",
            "File exists: gpt2/124M/model.ckpt.meta\n",
            "https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe gpt2/124M/vocab.bpe\n",
            "File exists: gpt2/124M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dir = os.path.join(\"gpt2\", \"124M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'gpt2/124M/model.ckpt'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "tf_ckpt_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables = tf.train.list_variables(tf_ckpt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name, _ = variables[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.train.load_variable(tf_ckpt_path, name).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "a.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50257, 768)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a[\"wte\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(a[\"blocks\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(768,)"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a[\"g\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a[\"wpe\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a[\"blocks\"][0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tf.train.latest_checkpoint??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.gpt_download import download_and_load_gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn(8, 128, 128)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x.mean(dim=-1, keepdim=True).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x.var??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (\n",
        "            0.5\n",
        "            * x\n",
        "            * (\n",
        "                1\n",
        "                + torch.tanh(\n",
        "                    torch.sqrt(torch.tensor(2.0 / torch.pi))\n",
        "                    * (x + 0.044715 * torch.pow(x, 3))\n",
        "                )\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tensor1 = torch.randn(3)\n",
        "tensor1.shape\n",
        "tensor2 = torch.randn(3)\n",
        "torch.matmul(tensor1, tensor2).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tensor1 = torch.randn(10, 3, 4)\n",
        "tensor2 = torch.randn(4)\n",
        "torch.matmul(tensor1, tensor2).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tensor1 = torch.randn(10, 3, 4)\n",
        "tensor2 = torch.randn(10, 4, 5)\n",
        "torch.matmul(tensor1, tensor2).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tensor1 = torch.randn(10, 3, 4)\n",
        "tensor2 = torch.randn(4)\n",
        "torch.matmul(tensor1, tensor2).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tensor1 = torch.randn(10, 4, 3, 4)\n",
        "tensor2 = torch.randn(10, 4, 4, 5)\n",
        "(tensor1 @ tensor2).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mask = torch.triu(torch.ones(10, 10), diagonal=1).bool()[:5, :5]\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attn_scores = torch.randn(1, 1, 5, 5)\n",
        "attn_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attn_scores.masked_fill_(mask, -torch.inf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mask_l = torch.tril(torch.ones(10, 10))[:5, :5]\n",
        "mask_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attn_scores_l = torch.randn(1, 1, 5, 5)\n",
        "attn_scores_l.masked_fill_(mask_l == 0, -torch.inf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.softmax??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.softmax(attn_scores_l, dim=-1)\n",
        "a\n",
        "dropout = nn.Dropout(0.3)\n",
        "print(a, dropout(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "0.6316 * (1.0 / 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a.shape[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # transpose (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # dot product for each head\n",
        "        attn_scores = torch.matmul(\n",
        "            queries, keys.transpose(2, 3)\n",
        "        )  # (b, num_heads, num_tokens, num_tokens) # double check this\n",
        "        mask = self.mask[:num_tokens, :num_tokens].bool()\n",
        "        attn_scores.masked_fill_(mask, -torch.inf)\n",
        "\n",
        "        scaled_attn_scores = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
        "        scaled_attn_scores = self.dropout(scaled_attn_scores)\n",
        "\n",
        "        # (b, num_heads, num_tokens, num_tokens) *\n",
        "        # (b, num_heads, num_tokens, head_dim) -> (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (scaled_attn_scores @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    Apply LN and residual connection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"],\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "        # self.norm1 = LayerNorm(cfg['emb_dim'])\n",
        "        # self.norm2 = LayerNorm(cfg['emb_dim'])\n",
        "        # self.drop_resid = nn.Dropout(cfg['drop_rate'])\n",
        "        self.sublayer1 = SublayerConnection(cfg[\"emb_dim\"], cfg[\"drop_rate\"])\n",
        "        self.sublayer2 = SublayerConnection(cfg[\"emb_dim\"], cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # might have some interesting consequences when we load weights\n",
        "        # attention block\n",
        "        x = self.sublayer1(x, self.att)\n",
        "        # FF block\n",
        "        x = self.sublayer2(x, self.ff)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "        self.final_ln = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # [batch_size, seq_len, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_ln(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l = [1, 2, 3, 4]\n",
        "\n",
        "\n",
        "def f(a, b, c, d):\n",
        "    print(a, b, c, d)\n",
        "\n",
        "\n",
        "f(*l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pos_emb = nn.Embedding(10, 12)\n",
        "pos_emb\n",
        "a = torch.arange(10)\n",
        "a\n",
        "pos_emb(a).shape, a.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.randn(3, 10)\n",
        "print(a[:, -1])\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Util functions\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (B, T) array of indices in current context\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)  # (B, T, vocab_size)\n",
        "\n",
        "        logits = logits[:, -1, :]  # (batch, vocab_size)\n",
        "        idx_next = torch.argmax(logits, dim=1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.randn(10, 4)\n",
        "b = torch.randn(10, 1)\n",
        "c = torch.cat((a, b), dim=-1)\n",
        "c.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch, Left: {left.shape}, right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "    encoded = torch.tensor(encoded).unsqueeze(0)\n",
        "    return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    tokens = token_ids.squeeze(0)\n",
        "    return tokenizer.decode(tokens.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params[\"wpe\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params[\"wte\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params[\"blocks\"][0][\"attn\"][\"c_attn\"][\"w\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = params[\"blocks\"][0][\"attn\"][\"c_attn\"][\"w\"]\n",
        "q, k, v = np.split(a, 3, axis=-1)\n",
        "q.shape, k.shape, v.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params[\"blocks\"][0][\"attn\"][\"c_attn\"][\"b\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = params[\"blocks\"][0][\"attn\"][\"c_attn\"][\"b\"]\n",
        "q, k, v = np.split(a, 3, axis=0)\n",
        "q.shape, k.shape, v.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params[\"blocks\"][0][\"attn\"].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params[\"blocks\"][0][\"attn\"][\"c_proj\"][\"b\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params[\"blocks\"][0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params[\"blocks\"][0][\"mlp\"][\"c_fc\"][\"w\"].shape, params[\"blocks\"][0][\"mlp\"][\"c_fc\"][\n",
        "    \"b\"\n",
        "].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params[\"blocks\"][0][\"mlp\"][\"c_proj\"][\"w\"].shape, params[\"blocks\"][0][\"mlp\"][\"c_proj\"][\n",
        "    \"b\"\n",
        "].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params[\"blocks\"][0][\"ln_1\"][\"g\"].shape, params[\"blocks\"][0][\"ln_1\"][\"b\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_weights_into_gpt(gpt: GPTModel, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        # Multi head Attention layer\n",
        "        q_w, k_w, v_w = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T\n",
        "        )\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T\n",
        "        )\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T\n",
        "        )\n",
        "\n",
        "        q_b, k_b, v_b = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b\n",
        "        )\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b\n",
        "        )\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T,\n",
        "        )\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"],\n",
        "        )\n",
        "\n",
        "        # FF layer\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T,\n",
        "        )\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
        "        )\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T,\n",
        "        )\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"],\n",
        "        )\n",
        "\n",
        "        # Norm layers\n",
        "        gpt.trf_blocks[b].sublayer1.norm.scale = assign(\n",
        "            gpt.trf_blocks[b].sublayer1.norm.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
        "        )\n",
        "        gpt.trf_blocks[b].sublayer1.norm.shift = assign(\n",
        "            gpt.trf_blocks[b].sublayer1.norm.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
        "        )\n",
        "        gpt.trf_blocks[b].sublayer2.norm.scale = assign(\n",
        "            gpt.trf_blocks[b].sublayer2.norm.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
        "        )\n",
        "        gpt.trf_blocks[b].sublayer2.norm.shift = assign(\n",
        "            gpt.trf_blocks[b].sublayer2.norm.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
        "        )\n",
        "\n",
        "        # Final norm and output layer weight\n",
        "        gpt.final_ln.scale = assign(gpt.final_ln.scale, params[\"g\"])\n",
        "        gpt.final_ln.shift = assign(gpt.final_ln.shift, params[\"b\"])\n",
        "        gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params[\"wte\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,  # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,  # Dropout rate\n",
        "    \"qkv_bias\": True,  # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.gpt_model import GPTModel, load_weights_into_gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=150,\n",
        "    context_size=1024,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize the model with pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHOOSE_MODEl = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Evert effort moves\"\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"drop_rate\": 0.0,\n",
        "    \"qkv_bias\": True,\n",
        "}\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "}\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "BASE_CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.gpt_download import download_and_load_gpt2\n",
        "from utils.gpt_model import (\n",
        "    generate_text_simple,\n",
        "    GPTModel,\n",
        "    load_weights_into_gpt,\n",
        "    text_to_token_ids,\n",
        "    token_ids_to_text,\n",
        ")\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "model_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_2 = \"\"\"\n",
        "    Is the following text 'spam'? Answer with 'yes' or 'no':\n",
        "    'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
        "    Answer with 'yes' or 'no'.\n",
        "\"\"\"\n",
        "print(text_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_2, tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        ")\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(list(model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# freeze the model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "num_classes = 2\n",
        "model.out_head = nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    if param.requires_grad:\n",
        "        print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model.final_ln.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = 0\n",
        "for param in model.parameters():\n",
        "    if param.requires_grad:\n",
        "        params += 1\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = tokenizer.encode(\"How are you doing\")\n",
        "inputs = torch.tensor(inputs).unsqueeze(0)\n",
        "inputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    output = model(inputs)\n",
        "\n",
        "print(output)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output[-1, :, :].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits = output[-1, :, :]\n",
        "label = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_batch)[:, -1, :]  # (batch_size, 2)\n",
        "\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)  # (batch_size)\n",
        "            num_examples += predicted_labels.shape[0]\n",
        "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    accuracy = (correct_predictions / num_examples) * 100\n",
        "    return f\"{accuracy:.2f}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "valid_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "print(train_accuracy, valid_accuracy, test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for input_batch, label_batch in train_loader:\n",
        "    # print(input_batch.to(device))\n",
        "    print(label_batch.shape)\n",
        "    logits = model(input_batch)[:, -1, :]\n",
        "    print(logits.shape)\n",
        "    loss = torch.nn.functional.cross_entropy(logits, label_batch)\n",
        "    print(label_batch)\n",
        "    print(loss)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target = torch.randint(5, (3,), dtype=torch.int64)\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)[:, -1, :]\n",
        "    loss = F.cross_entropy(logits, target_batch)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    loss = 0.0\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        # print(input_batch.shape, target_batch.shape)\n",
        "        if i < num_batches:\n",
        "            loss_batch = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss += loss_batch.item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calc_loss_loader(train_loader, model, device, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, 5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, 5)\n",
        "    test_loss = calc_loss_loader(test_loader, model, device, 5)\n",
        "\n",
        "print(f\"Train loss: {train_loss:.3f}\")\n",
        "print(f\"valid loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Finetune the model to improve the loss value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_classifier_simple(\n",
        "    model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter\n",
        "):\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            examples_seen += input_batch.shape[0]\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional eval step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter\n",
        "                )\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(\n",
        "                    f\"Epoch {epoch + 1} step {global_step:06d}, train_loss: {train_loss:.3f}, val_loss: {val_loss:.3f}\"\n",
        "                )\n",
        "\n",
        "        # calculate accuracy after evrry epoch\n",
        "        train_accuracy = calc_accuracy_loader(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        val_accuracy = calc_accuracy_loader(\n",
        "            val_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "        print(f\"train accuracy: {train_accuracy}, val_accuracy: {val_accuracy}\")\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len([p for p in model.parameters() if p.requires_grad == True])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs=num_epochs,\n",
        "    eval_freq=50,\n",
        "    eval_iter=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "end_time = time.time()\n",
        "train_duration = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {train_duration:.2f} mins\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(train_losses), len(val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(train_accs), len(val_accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(train_loader), train_loader.batch_size, len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(5 * 130) / 50  # length of loss array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples_seen, (130 * 8 * 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs_tensor = torch.linspace(1, num_epochs, len(train_losses))\n",
        "epochs_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples_seen_tensor = torch.linspace(1, examples_seen, len(train_losses))\n",
        "examples_seen_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_losses, len(train_losses), examples_seen_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_values(epochs, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    ax1.plot(epochs, train_values, label=f\"Training {label}\")\n",
        "    ax1.plot(epochs, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(label.capitalize())\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2 = ax1.twiny()  # create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(examples_seen, train_values, alpha=0)  # invisible plot\n",
        "    ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(f\"{label}-plot.pdf\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs_tensor = torch.linspace(1, num_epochs, len(train_accs))\n",
        "examples_seen_tensor = torch.linspace(1, examples_seen, len(train_accs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples_seen_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_values(\n",
        "    epochs_tensor,\n",
        "    examples_seen_tensor,\n",
        "    [float(p) for p in train_accs],\n",
        "    [float(p) for p in val_accs],\n",
        "    label=\"Accuracy\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[float(p) for p in val_accs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute accuracy over the whole dataset\n",
        "\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "train_accuracy, val_accuracy, test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use LLM as spam classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.pos_emb.weight.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_review(\n",
        "    text, model, tokenizer, device, max_length=None, pad_token_id=50256\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    supported_context_length = model.pos_emb.weight.shape[\n",
        "        0\n",
        "    ]  # mistake in original implementation\n",
        "\n",
        "    if max_length is None:\n",
        "        max_length = supported_context_length\n",
        "    else:\n",
        "        max_length = min(max_length, supported_context_length)\n",
        "\n",
        "    input_ids = input_ids[:max_length]\n",
        "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "    # print(input_ids)\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(\n",
        "        0\n",
        "    )  # add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)[\n",
        "            :, -1, :\n",
        "        ]  # logits of last output token Shape should be (1, 2)\n",
        "\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "    return \"not spam\" if predicted_label == 0 else \"spam\", logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_1 = \"How are you doing\"\n",
        "\n",
        "classify_review(text_1, model, tokenizer, device, max_length=train_dataset.max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_2 = (\n",
        "    \"Hey, just wanted to check if we're still on\" \" for dinner tonight? Let me know!\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    classify_review(\n",
        "        text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset.max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_1 = (\n",
        "    \"You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    classify_review(\n",
        "        text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"spam_review_classifier.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_state_dict = torch.load(\"spam_review_classifier.pth\")\n",
        "model.load_state_dict(model_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# max_length is quite important, having too many padding tokens will lead all inputs to be classified as spam."
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "2faf2d7c-0332-49e3-b8d4-6c62d00fc5ee",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "bento_kernel_default"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  }
}
