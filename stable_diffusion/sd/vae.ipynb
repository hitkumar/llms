{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# from decoder import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip import CLIPLayer, CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Encoder(nn.Sequential):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            # (Batch_size, Channel, Height, Width) -> (B, 128, Height, Weight)\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "\n",
    "            # (B, 128, Height, Weight) -> (B, 128, Height, Weight)\n",
    "            # VAE_ResidualBlock(128, 128),\n",
    "            # VAE_ResidualBlock(128, 128),\n",
    "\n",
    "            # (B, 128, Height, Weight) -> (B, 128, Height / 2, Weight / 2) \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # (B, 128, Height / 2, Weight / 2) -> (B, 256, Height / 2, Weight / 2) \n",
    "            # VAE_ResidualBlock(128, 256),\n",
    "            # VAE_ResidualBlock(256, 256),\n",
    "\n",
    "            # (B, 256, Height / 4, Weight / 4) \n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0),\n",
    "            # (B, 512, Height / 4, Weight / 4) \n",
    "            # VAE_ResidualBlock(256, 512),\n",
    "            # VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (B, 512, Height / 8, Weight / 8) \n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=0),\n",
    "            # (B, 512, Height / 8, Weight / 8) \n",
    "            # VAE_ResidualBlock(512, 512),\n",
    "            # VAE_ResidualBlock(512, 512),\n",
    "            # VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (B, 512, Height / 8, Weight / 8) \n",
    "            # VAE_AttentionBlock(512)\n",
    "\n",
    "            # VAE_ResidualBlock(512, 512),\n",
    "            # VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (B, 512, Height / 8, Weight / 8) \n",
    "            nn.GroupNorm(32, 512),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            # (B, 8, Height / 8, Weight / 8) \n",
    "            nn.Conv2d(512, 8, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(8, 8, kernel_size=1, padding=0),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.tensor, noise: torch.tensor) -> torch.tensor:\n",
    "        # x: (B, C, H, W)\n",
    "        # noise: (B, out_channels, H / 8, W / 8)\n",
    "        for module in self:\n",
    "            if getattr(module, 'stride', None) == (2, 2):\n",
    "                # (padding - left, right, top, bottom) \n",
    "                x = F.pad(x, (0, 1, 0, 1))\n",
    "                x = module(x)\n",
    "        \n",
    "        # now we have to return the mean and var since this is a VAE\n",
    "        # output of last layer is (B, 8, Height / 8, Weight / 8) -> two tensors of shape (B, 4, Height / 8, width / 8)\n",
    "        # TODO: Why are we doing this to find mean and variance?\n",
    "        mean, log_variance = torch.chunk(x, 2, dim=1)\n",
    "        # (B, 4, Height / 8, width / 8)\n",
    "        log_variance = torch.clamp(log_variance, -30, 20)        \n",
    "        variance = log_variance.exp()\n",
    "        std_dev = variance.sqrt()\n",
    "\n",
    "        # z = N(0, 1) -> N(mean, variance) = X?\n",
    "        x = mean + std_dev * noise\n",
    "\n",
    "        # scale the output (not sure why)\n",
    "        x *= 0.18215\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from attention import SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 8, 8)\n",
    "conv = nn.Conv2d(1, 8, kernel_size=1, padding=0)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        # x: (B, in_channels, height, width)\n",
    "        residue = x\n",
    "        x = self.groupnorm_1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.groupnorm_2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_2(x)\n",
    "        return x + self.residual_layer(residue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(32, channels)\n",
    "        self.attention = SelfAttention(1, channels)\n",
    "    \n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        residue = x\n",
    "        n,c,h,w = x.shape\n",
    "        x = x.view(n, c, h * w)\n",
    "        # (B, H*w, Features)\n",
    "        x = x.transpose(-1, -2)\n",
    "        # do the attention between features\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # (B, Features, H*w)\n",
    "        x = x.transpose(-1, -2)\n",
    "        # convert to original shape\n",
    "        x = x.view(n, c, h, w)\n",
    "        x = x + residue\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build attention - both self attention and cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8, 3, 3)\n",
    "q, k, v = x.chunk(3, dim=-1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(8, 3, 16, 4)\n",
    "v = torch.randn(8, 3, 16, 4)\n",
    "torch.matmul(q, v.transpose(-2, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_embed: int, in_proj_bias=True, out_proj_bias=True):\n",
    "        '''\n",
    "        d_embed is number of channels/features per pixel here\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_embed // n_heads\n",
    "    \n",
    "    def forward(self, x, causual_mask=False):\n",
    "        '''\n",
    "        X is like # (B, H*w, Features)\n",
    "        '''\n",
    "        input_shape = x.shape\n",
    "        batch_size, sequence_length, d_embed = input_shape\n",
    "        interim_shape = (batch_size, sequence_length, self.n_heads, self.head_dim)\n",
    "\n",
    "        # (B, S, D) -> (B, S, 3*D) -> 3 tensors of shape (B, S, D)\n",
    "        q,k ,v = self.in_proj(x).chunk(3, dim=-1)\n",
    "\n",
    "        # (B, S, H, Dim/H) -> (B, H, S, Dim/H)\n",
    "        q = q.view(interim_shape).transpose(1, 2)\n",
    "        k = k.view(interim_shape).transpose(1, 2)\n",
    "        v = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "        # (B, H, S, Dim/H) * (B, H, Dim/H, S) -> (B, H, S, S)\n",
    "        weight = torch.matmul(q, k.tranpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        if causual_mask:\n",
    "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
    "            weight.masked_fill_(mask, -torch.inf)\n",
    "        \n",
    "        # (B, H, S, S)\n",
    "        weights = F.softmax(weight, dim=-1)\n",
    "        # (B, H, S, S)\n",
    "        # (B, H, S, Dim/H) -> (B, H, S, Dim/H)\n",
    "        output = weight @ v\n",
    "        output = output.transpose(1, 2)\n",
    "        # (B, S, dim)\n",
    "        output = output.view(input_shape)\n",
    "        # (B, S, dim)\n",
    "        output = self.out_proj(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(4, 4, kernel_size=1, padding=0),\n",
    "            nn.Conv2d(4, 512, kernel_size=3, padding=1),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            VAE_AttentionBlock(512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (B, 512, Height / 8, Width / 8) -> (64, 64) for us\n",
    "\n",
    "            # (B, 512, Height / 8, Width / 8) -> # (B, 512, Height / 4, Width / 4)\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (B, 512, Height / 4, Width / 4) -> # (B, 512, Height / 2, Width / 2) (256, 256) for us\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            VAE_ResidualBlock(512, 256),\n",
    "            VAE_ResidualBlock(256, 256),\n",
    "            VAE_ResidualBlock(256, 256),\n",
    "\n",
    "            # (B, 246, Height / 2, Width / 2) -> # (B, 128, Height, Width) (512, 512) for us\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            VAE_ResidualBlock(256, 128),\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "\n",
    "            nn.GroupNorm(32, 128),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            # (B, 128, Height, Width) -> (B, 3, Height, Width)\n",
    "            nn.Conv2d(128, 3, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        # x: (B, 4, H / 8, W / 8)\n",
    "        x /= 0.18215\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "        \n",
    "        # (B, 3, H, W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEmbedding(nn.Module):\n",
    "    def __init__(self, n_vocab: int, n_embd: int, n_token: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_embd)\n",
    "        self.position_embedding = nn.Parameter(torch.zeros((n_token, n_embd)))\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # (B, S) -> (B, S, D)\n",
    "        x = self.token_embedding(tokens)\n",
    "        x += self.position_embedding\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_embedding = CLIPEmbedding(1024, 16, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.arange(20)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_embedding(tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPLayer(nn.Module):\n",
    "    def __init__(self, n_head: int, n_embd: int):\n",
    "        super().__init__()\n",
    "        self.layernorm_1 = nn.LayerNorm(n_embd)\n",
    "        self.attention = SelfAttention(n_head, n_embd)\n",
    "        self.layernorm_2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.linear1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.linear2 = nn.Linear(4 * n_embd, n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layernorm_1(x), causal_mask=True)\n",
    "        x = self.linear1(self.layernorm_2(x))\n",
    "        x = x * torch.sigmoid(1.702 * x) # QuickGELU activation function\n",
    "        x = x + self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    '''\n",
    "    Returns an embedding for every token in the input sequence.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = CLIPEmbedding(49408, 768, 77)\n",
    "        self.layers = nn.ModuleList([\n",
    "            CLIPLayer(12, 768) for i in range(12)\n",
    "        ])\n",
    "        self.layernorm = nn.LayerNorm(768)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tokens = tokens.type(torch.long)\n",
    "\n",
    "        # (B, S) -> (B, S, Dim)\n",
    "        state = self.embedding(tokens)\n",
    "        for layer in self.layers:\n",
    "            state = layer(state)\n",
    "        output = self.layernorm(state)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8, 3, 16, 16)\n",
    "conv_1 = nn.Conv2d(3, 10, kernel_size=3, padding=1)\n",
    "conv_1(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.linspace(0, 10, 100, dtype=torch.float32) ** 2\n",
    "# beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer(\"../data/tokenizer_vocab.json\", merges_file=\"../data/tokenizer_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(\"How are you doing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_layer = CLIP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_layer(torch.tensor(tokens)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/htkumar/anaconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_file = \"../data/v1-5-pruned-emaonly.ckpt\"\n",
    "state_dict = model_converter.load_from_standard_weights(model_file, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['diffusion', 'encoder', 'decoder', 'clip'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip import CLIP\n",
    "from encoder import VAE_Encoder\n",
    "from decoder import VAE_Decoder\n",
    "from diffusion import Diffusion\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip = CLIP().to('cpu')\n",
    "clip.load_state_dict(state_dict['clip'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = VAE_Encoder().to('cpu')\n",
    "encoder.load_state_dict(state_dict['encoder'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = VAE_Decoder().to('cpu')\n",
    "decoder.load_state_dict(state_dict['decoder'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion = Diffusion().to('cpu')\n",
    "diffusion.load_state_dict(state_dict['diffusion'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_start = .00085\n",
    "beta_end = .0120\n",
    "betas = torch.linspace(beta_start ** 0.5, beta_end ** 0.5, 1000, dtype=torch.float32) ** 2\n",
    "# betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linspace(1, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3, 4])\n",
    "torch.cumprod(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.arange(0, 10)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(99, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(np.arange(0, 20)[::-1].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(19, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_ratio = 1000 // 50\n",
    "timesteps = (np.arange(0, 50) * step_ratio).round()[::-1].copy().astype(np.int64)\n",
    "timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(49, -1, -1).view(2, 25).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMSampler:\n",
    "    def __init__(self, generator: torch.Generator, num_training_steps=1000, beta_start=0.00085, beta_end=0.0120):\n",
    "        self.betas = torch.linspace(beta_start ** 0.5, beta_end ** 0.5, num_training_steps, dtype=torch.float32) ** 2\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.one = torch.tensor(1.0)\n",
    "\n",
    "        self.generator = generator\n",
    "        self.num_train_timesteps = num_training_steps\n",
    "        self.timesteps = torch.arange(num_training_steps - 1, -1, -1)\n",
    "    \n",
    "    def set_inference_timesteps(self, num_inference_steps=50):\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        step_ratio = self.num_train_timesteps // self.num_inference_steps\n",
    "        self.step_ratio = step_ratio\n",
    "        # In decreasing order 980, 960, 940 ....\n",
    "        self.timesteps = torch.arange(num_inference_steps - 1, -1, -1) * step_ratio\n",
    "    \n",
    "    def _get_previous_timestep(self, timestep: int) -> int:\n",
    "        prev_t = timestep - self.step_ratio\n",
    "        return prev_t\n",
    "    \n",
    "    def add_noise(self, original_samples: torch.tensor, timesteps: torch.tensor):\n",
    "        sqrt_alpha_prod = self.alphas_cumprod[timesteps] ** 0.5\n",
    "        # TODO: Is this needed, verify the shapes\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n",
    "            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "        \n",
    "        variance = (1 - self.alphas_cumprod[timesteps]) ** 0.5\n",
    "        variance = variance.flatten()\n",
    "        while len(variance.shape) < len(original_samples.shape):\n",
    "            variance = variance.unsqueeze(-1)\n",
    "        \n",
    "        # Like in eq(4) of DDPm paper, q(x_t/x_0) can be obtained\n",
    "        noise = torch.randn(original_samples.shape, generator=self.generator, device=original_samples.device, dtype=original_samples.dtype)\n",
    "        noisy_samples = sqrt_alpha_prod * original_samples + variance * noise\n",
    "        return noisy_samples\n",
    "    \n",
    "    def set_strength(self, strength=1):\n",
    "        '''\n",
    "        More noise: output will be further from input\n",
    "        less noise: outpout will be closer to input image\n",
    "        '''\n",
    "        start_step = self.num_inference_steps - int(self.num_inference_steps * strength)\n",
    "        self.timesteps = self.timesteps[start_step:]\n",
    "        self.start_step = start_step\n",
    "    \n",
    "    def step(self, t: int, latents: torch.tensor, model_output: torch.tensor):\n",
    "        '''\n",
    "        Remove noise from the latents and get latent at timestep (t-1)\n",
    "        latents is x(t) at timestep t, model_output is the predicted noise\n",
    "        '''\n",
    "        prev_t = self._get_previous_timestep(t)\n",
    "\n",
    "        # We use formula 6 and 7 of the paper, calculate alphas and beta first\n",
    "        alpha_prod_t = self.alphas_cumprod[t]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
    "        current_alpha_t = alpha_prod_t / alpha_prod_t_prev\n",
    "        current_beta_t = 1 - current_alpha_t\n",
    "\n",
    "        # Now calculate x_0\n",
    "        pred_original_sample = (latents - (beta_prod_t ** 0.5) * model_output) / (alpha_prod_t ** 0.5)\n",
    "        predicted_original_sample_coeff = ((alpha_prod_t_prev ** 0.5) * current_beta_t) / beta_prod_t\n",
    "        current_sample_coeff = (current_alpha_t ** (0.5) * beta_prod_t_prev) / beta_prod_t\n",
    "\n",
    "        prev_sample_mean = predicted_original_sample_coeff * pred_original_sample + current_sample_coeff * latents\n",
    "\n",
    "        # variance\n",
    "        variance = 0\n",
    "        if t > 0:\n",
    "            noise = torch.randn(model_output.shape, generator=self.generator, device=model_output.device, dtype=model_output.dtype)\n",
    "            variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t\n",
    "            variance = torch.clamp(variance, min=1e-20)\n",
    "            variance = variance ** 0.5\n",
    "        \n",
    "        pred_prev_sample = pred_prev_sample + variance * noise\n",
    "        return pred_prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm_sampler = DDPMSampler(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"../images/dog.jpg\")\n",
    "img_tensor = torch.tensor(np.array(img))\n",
    "img_tensor.shape\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = ((img_tensor / 255.0) * 2.0) - 1.0\n",
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = [0, 10, 50, 75, 100, 250, 500, 750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = img_tensor.repeat(len(noise_levels), 1, 1, 1)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.tensor(noise_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_imgs = []\n",
    "epsilons = torch.randn(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ts)):\n",
    "    a_hat = ddpm_sampler.alphas_cumprod[ts[i]]\n",
    "    noise_imgs.append(\n",
    "        # Equation 4 of the paper, all images in the batch are identical\n",
    "        math.sqrt(a_hat) * batch[i] + math.sqrt(1 - a_hat) * epsilons[i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_imgs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_imgs = torch.stack(noise_imgs, dim=0)\n",
    "noise_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_imgs = (noise_imgs.clamp(-1, 1) + 1) / 2\n",
    "noise_imgs = (noise_imgs * 255).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_imgs[7].squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img = Image.fromarray(noise_imgs[4].squeeze(0).numpy(), 'RGB')\n",
    "# display_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5, 6], dtype=torch.float32).view(1, 2, 3); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = F.interpolate(x, scale_factor=2, mode='nearest'); y\n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a.append(1)\n",
    "a.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from attention import SelfAttention, CrossAttention\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(n_embed, 4 * n_embed)\n",
    "        self.linear_2 = nn.Linear(4 * n_embed, 4 * n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x : (1, 320)\n",
    "        x = F.silu(self.linear_1(x))\n",
    "        # (1, 1280)\n",
    "        return self.linear_2(x)\n",
    "\n",
    "\n",
    "class UNET_ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_time=1280):\n",
    "        super().__init__()\n",
    "        self.groupnorm_feature = nn.GroupNorm(32, in_channels)\n",
    "        self.conv_feature = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.linear_time = nn.Linear(n_time, out_channels)\n",
    "\n",
    "        self.groupnorm_merged = nn.GroupNorm(32, out_channels)\n",
    "        self.conv_merged = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, feature, time):\n",
    "        # feature: (B, C, H, W)\n",
    "        # time: (1, 1280)\n",
    "        residue = feature\n",
    "\n",
    "        # (B, in_channels, H, W)\n",
    "        feature = self.groupnorm_feature(feature)\n",
    "        feature = F.silu(feature)\n",
    "        # (B, out_channels, H, W)\n",
    "        feature = self.conv_feature(feature)\n",
    "\n",
    "        # (1, 1280)\n",
    "        time = F.silu(time)\n",
    "        # (1, 1280) -> (1. out_channels)\n",
    "        time = self.linear_time(time)\n",
    "\n",
    "        # Add height and width dimension to time\n",
    "        # (B, out_channels, H, W)\n",
    "        merged = feature + time.unsqueeze(-1).unsqueeze(-1)\n",
    "        merged = self.groupnorm_merged(merged)\n",
    "        merged = F.silu(merged)\n",
    "        merged = self.conv_merged(merged)\n",
    "\n",
    "        return merged + self.residual_layer(residue)\n",
    "\n",
    "\n",
    "class UNET_AttentionBlock(nn.Module):\n",
    "    def __init__(self, n_head: int, n_embed: int, d_context: int=768):\n",
    "        super().__init__()\n",
    "        channels = n_head * n_embed\n",
    "\n",
    "        self.groupnorm = nn.GroupNorm(32, channels, eps=1e-6)\n",
    "        self.conv_input = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
    "\n",
    "        self.layernorm_1 = nn.LayerNorm(channels)\n",
    "        self.attention_1 = SelfAttention(n_head, channels, in_proj_bias=False)\n",
    "        self.layernorm_2 = nn.LayerNorm(channels)\n",
    "        self.attention_2 = CrossAttention(n_head, channels, d_context, in_proj_bias=False)\n",
    "        self.layernorm_3 = nn.LayerNorm(channels)\n",
    "        self.linear_geglu_1 = nn.Linear(channels, 4 * channels * 2)\n",
    "        self.linear_geglu_2 = nn.Linear(4 * channels, channels)\n",
    "\n",
    "        self.conv_output = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, image, context):\n",
    "        # image: (B, F, H, W)\n",
    "        # context: (B, seq_len, Dim)\n",
    "        residue = image\n",
    "\n",
    "        # (B, F, H, W)\n",
    "        image = self.groupnorm(image)\n",
    "        image = self.conv_input\n",
    "\n",
    "        n, c, h, w = image.shape\n",
    "\n",
    "        # (B, H*W, F)\n",
    "        image = image.view((n, c, h * w)).transpose(1, 2)\n",
    "\n",
    "        # Self attention\n",
    "        # (B, H*W, F)\n",
    "        image = image + self.attention_1(self.layernorm_1(image))\n",
    "\n",
    "        # Cross Attention\n",
    "        # (B, H*W, F)\n",
    "        image = image + self.attention_2(self.layernorm_2(image), context)\n",
    "\n",
    "        residue_tmp = image\n",
    "        # (B, H*W, F)\n",
    "        image = self.layernorm_3(image)\n",
    "\n",
    "        # (B, H*W, F) -> two tensors of dim (B, H*W, F * 4)\n",
    "        image, gate = self.linear_geglu_1(image).chunk(2, dim=-1)\n",
    "\n",
    "        # (B, H*W, F * 4)\n",
    "        image = image * F.gelu(gate)\n",
    "        # (B, H*W, F)\n",
    "        image = self.linear_geglu_2(image)\n",
    "\n",
    "        image += residue_tmp\n",
    "        # (B, F, H, W)\n",
    "        image = image.transpose(-1, -2).view(image.shape)\n",
    "\n",
    "        # residual connection -> (B, F, H, W)\n",
    "        return residue + self.conv_output(image)\n",
    "    \n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # X : (B, F, H, W)\n",
    "\n",
    "        # (B, F, H * 2, W * 2)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        return self.conv(x)\n",
    "\n",
    "class SwitchSequential(nn.Sequential):\n",
    "    def forward(self, image, context, time):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, UNET_AttentionBlock):\n",
    "                image = layer(layer, context)\n",
    "            elif isinstance(layer, UNET_ResidualBlock):\n",
    "                image = layer(image, time)\n",
    "            else:\n",
    "                image = layer(image)\n",
    "    \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = UNET_AttentionBlock(8, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = nn.ModuleList([\n",
    "            # (B, 4, H / 8, W / 8) -> (B, 320, H / 8, W / 8)\n",
    "            SwitchSequential(nn.Conv2d(4, 320, kernel_size=3, padding=1)),\n",
    "            # (B, 320, H / 8, W / 8)\n",
    "            SwitchSequential(UNET_ResidualBlock(320, 320), UNET_AttentionBlock(8, 40)),\n",
    "            SwitchSequential(UNET_ResidualBlock(320, 320), UNET_AttentionBlock(8, 40)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in encoders:\n",
    "    print(layer.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # decrease image dimension and increase the number of channels, input to this is the output of VAEEncoder (latent)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            # (B, 4, H / 8, W / 8) -> (B, 320, H / 8, W / 8)\n",
    "            SwitchSequential(nn.Conv2d(4, 320, kernel_size=3, padding=1)),\n",
    "            # (B, 320, H / 8, W / 8)\n",
    "            SwitchSequential(UNET_ResidualBlock(320, 320), UNET_AttentionBlock(8, 40)),\n",
    "            SwitchSequential(UNET_ResidualBlock(320, 320), UNET_AttentionBlock(8, 40)),\n",
    "\n",
    "            # (B, 320, H / 16, W / 16)\n",
    "            SwitchSequential(nn.Conv2d(320, 320, kernel_size=3, padding=1, stride=2)),\n",
    "            # (B, 640, H / 16, W / 16)\n",
    "            SwitchSequential(UNET_ResidualBlock(320, 640), UNET_AttentionBlock(8, 80)),\n",
    "            SwitchSequential(UNET_ResidualBlock(320, 640), UNET_AttentionBlock(8, 80)),\n",
    "\n",
    "            # (B, 640, H / 32, W / 32)\n",
    "            SwitchSequential(nn.Conv2d(640, 640, kernel_size=3, padding=1, stride=2)),\n",
    "            # (B, 1280, H / 32, W / 32)\n",
    "            SwitchSequential(UNET_ResidualBlock(640, 1280), UNET_AttentionBlock(8, 160)),\n",
    "            SwitchSequential(UNET_ResidualBlock(1280, 1280), UNET_AttentionBlock(8, 160)),\n",
    "\n",
    "            # (B, 1280, H / 64, W / 64)\n",
    "            SwitchSequential(nn.Conv2d(1280, 1280, kernel_size=3, padding=1, stride=2)),\n",
    "            # (B, 1280, H / 64, W / 64)\n",
    "            SwitchSequential(UNET_ResidualBlock(1280, 1280)),\n",
    "            SwitchSequential(UNET_ResidualBlock(1280, 1280)),\n",
    "        ])\n",
    "\n",
    "        self.bottleneck = SwitchSequential(\n",
    "            # (B, 1280, H / 64, W / 64)\n",
    "            UNET_ResidualBlock(1280, 1280),\n",
    "            UNET_AttentionBlock(8, 160),\n",
    "            UNET_ResidualBlock(1280, 1280),\n",
    "        )\n",
    "\n",
    "        self.decoders = nn.ModuleList([\n",
    "            # (B, 1280, H / 64, W / 64)\n",
    "            SwitchSequential(UNET_ResidualBlock(2560, 1280)), # 1280 concat\n",
    "            # (B, 1280, H / 64, W / 64)\n",
    "            SwitchSequential(UNET_ResidualBlock(2560, 1280)),  # 1280 concat\n",
    "\n",
    "            # (B, 1280, H / 32, W / 32)\n",
    "            SwitchSequential(UNET_ResidualBlock(2560, 1280), UpSample(1280)), # 1280 concat\n",
    "\n",
    "            # (B, 1280, H / 32, W / 32)\n",
    "            SwitchSequential(UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)), # 1280 concat\n",
    "            SwitchSequential(UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)), # 1280 concat\n",
    "\n",
    "            # (B, 1280, H / 16, W / 16)\n",
    "            SwitchSequential(UNET_ResidualBlock(1920, 1280), UNET_AttentionBlock(8, 160), UpSample(1280)), # 640 concat\n",
    "            # (B, 640, H / 16, W / 16)\n",
    "            SwitchSequential(UNET_ResidualBlock(1920, 640), UNET_AttentionBlock(8, 80)), # 640 concat\n",
    "            SwitchSequential(UNET_ResidualBlock(1280, 640), UNET_AttentionBlock(8, 80)), # 640 concat\n",
    "\n",
    "            # (B, 640, H / 8, W / 8)\n",
    "            SwitchSequential(UNET_ResidualBlock(960, 640), UNET_AttentionBlock(8, 160), UpSample(640)), # 320 concat\n",
    "            SwitchSequential(UNET_ResidualBlock(960, 320), UNET_AttentionBlock(8, 40)), # 320 concat\n",
    "            SwitchSequential(UNET_ResidualBlock(640, 320), UNET_AttentionBlock(8, 40)), # 320 concat\n",
    "            # (B, 320, H / 8, W / 8)\n",
    "            SwitchSequential(UNET_ResidualBlock(640, 320), UNET_AttentionBlock(8, 40)), # 320 concat\n",
    "        ])\n",
    "    \n",
    "    def forward(self, image, context, time):\n",
    "        '''\n",
    "        image: (B, 4, H / 8, W / 8)\n",
    "        context: (B, s_len, Dim)\n",
    "        time: (1, 1280)\n",
    "        '''\n",
    "\n",
    "        skip_connections = []\n",
    "        for layers in self.encoders:\n",
    "            image = layers(image, context, time)\n",
    "            skip_connections.append(image)\n",
    "        \n",
    "        image = self.bottleneck(image, context, time)\n",
    "        \n",
    "        for layer in self.decoders:\n",
    "            # this concat increases the number of images sent to decoder layers\n",
    "            # \n",
    "            image = torch.cat((image, skip_connections.pop()), dim=1)\n",
    "            image = layer(image, context, time)\n",
    "\n",
    "            '''\n",
    "            encoder last layer output is (B, 1280, H / 64, W / 64)\n",
    "            image from bottleneck layer output is (B, 1280, H / 64, W / 64)\n",
    "            These are concat in first decoder layer so first dim is 2560\n",
    "            '''\n",
    "        # (B, 320, H / 8, W / 8)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_OutputLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(32, in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # image: (B, 320, H / 8, W / 8)\n",
    "        image = self.groupnorm(image)\n",
    "        image = F.silu(image)\n",
    "        # (B, 4, H / 8, W / 8)\n",
    "        image = self.conv(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time_embedding = TimeEmbedding(320)\n",
    "        self.unet = UNET()\n",
    "        self.final = UNET_OutputLayer(320, 4)\n",
    "    \n",
    "    def forward(self, latent, context, time):\n",
    "        '''\n",
    "        latent is (B, 4, H / 8, W / 8)\n",
    "        context: (B, seq_len, dim)\n",
    "        time: (1, 320)\n",
    "        '''\n",
    "        time = self.time_embedding(time)\n",
    "        # (B, 4, H / 8, W / 8)\n",
    "        output = self.final(self.unet(latent, context, time))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
